# train_final_torch.py (ìˆ˜ì •ë³¸)

import wandb
import pandas as pd
import numpy as np
import os
import cv2
import time
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from types import SimpleNamespace
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# --- GPU ì„¤ì • ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# --- ê²½ë¡œ ì„¤ì • (ì „ì—­ ë³€ìˆ˜) ---
CSV_FILE_PATH = "./FINAL_master_summary_28828.csv" 
NPY_DIR = "./FINAL_NPY_28828"
PNG_DIR = "./3_audio_spectrograms"
IMG_HEIGHT, IMG_WIDTH = 128, 128
NPY_SEQ_LENGTH, NPY_FEATURES = 90, 5 

# --- Dataset í´ë˜ìŠ¤ (ë™ì¼) ---
class MultiModalRamDataset(Dataset):
    def __init__(self, df, npy_dir, png_dir, img_dims, npy_dims, scaler, mode='Train'):
        self.df = df.reset_index(drop=True)
        self.npy_dir = npy_dir
        self.png_dir = png_dir
        self.img_height, self.img_width = img_dims
        self.seq_len, self.n_features = npy_dims
        self.scaler = scaler
        self.cached_data = []
        
        # ë°ì´í„° ë¡œë”© ë¡œê·¸ëŠ” ë„ˆë¬´ ë§ìœ¼ë©´ ë³´ê¸° í˜ë“œë‹ˆ ê°„ë‹¨íˆ ì²˜ë¦¬
        # print(f"[{mode}] ë°ì´í„° ë¡œë“œ ì¤‘...") 
        
        for i in range(len(self.df)):
            video_id = self.df.loc[i, 'video_id']
            try:
                png_path = os.path.join(self.png_dir, f"{video_id}.png")
                img = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)
                if img is None: raise FileNotFoundError
                img = cv2.resize(img, (self.img_width, self.img_height))
                img_normalized = img / 255.0
                img_tensor = torch.tensor(img_normalized, dtype=torch.float32).unsqueeze(0)
            except:
                img_tensor = torch.zeros((1, self.img_height, self.img_width), dtype=torch.float32)
            
            try:
                npy_path = os.path.join(self.npy_dir, f"{video_id}.npy")
                data = np.load(npy_path, allow_pickle=True).item()
                mouth_data = np.stack([
                    data['mouth']['laplacian_mean'], data['mouth']['laplacian_var'],
                    data['mouth']['light_intensity_mean'], data['mouth']['light_intensity_change'],
                    data['mouth']['area']
                ], axis=1)
                mouth_data_scaled = self.scaler.transform(mouth_data.reshape(-1, self.n_features))
                curr_len = mouth_data_scaled.shape[0]
                padded_data = np.zeros((self.seq_len, self.n_features))
                if curr_len > self.seq_len: padded_data = mouth_data_scaled[:self.seq_len, :]
                else: padded_data[:curr_len, :] = mouth_data_scaled
                npy_tensor = torch.tensor(padded_data, dtype=torch.float32)
            except:
                npy_tensor = torch.zeros((self.seq_len, self.n_features), dtype=torch.float32)
            
            self.cached_data.append((img_tensor, npy_tensor))
            
    def __len__(self): return len(self.cached_data)
    def __getitem__(self, index):
        img_tensor, npy_tensor = self.cached_data[index]
        return (img_tensor, npy_tensor), (img_tensor, npy_tensor)

# --- Model í´ë˜ìŠ¤ (ë™ì¼) ---
class MultiModalAutoencoder(nn.Module):
    def __init__(self, cfg):
        super(MultiModalAutoencoder, self).__init__()
        self.cfg = cfg
        
        # CNN ëª¨ë¸ ì„ íƒ (Sweep íŒŒë¼ë¯¸í„° ëŒ€ì‘)
        # configì— cnn_model ê°’ì´ ë¬¸ìì—´ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ ë¶„ê¸° ì²˜ë¦¬
        if getattr(cfg, 'cnn_model', 'AlexNet_Mini') == 'AlexNet_Mini':
             self.cnn_encoder = nn.Sequential(
                nn.Conv2d(1, 32, kernel_size=5, stride=2, padding=0), nn.ReLU(), nn.MaxPool2d(kernel_size=2),
                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2),
                nn.Flatten(), nn.Linear(64 * 15 * 15, cfg.cnn_latent_dim), nn.ReLU()
            )
        else: 
            # ë‹¤ë¥¸ ëª¨ë¸ì¸ ê²½ìš° ê¸°ë³¸ êµ¬ì¡° (ì˜ˆì‹œ)
            self.cnn_encoder = nn.Sequential(
                nn.Conv2d(1, 16, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),
                nn.Flatten(), nn.Linear(16 * 64 * 64, cfg.cnn_latent_dim), nn.ReLU()
            )

        # RNN ì¸ì½”ë”
        self.rnn_encoder = nn.LSTM(input_size=NPY_FEATURES, hidden_size=cfg.rnn_units, batch_first=True)
        
        # Bottleneck
        self.bottleneck = nn.Sequential(nn.Linear(cfg.cnn_latent_dim + cfg.rnn_units, cfg.bottleneck_dim), nn.ReLU())
        
        # Decoders
        self.rnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, cfg.rnn_units)
        self.rnn_decoder = nn.LSTM(input_size=cfg.rnn_units, hidden_size=cfg.rnn_units, batch_first=True)
        self.rnn_output_layer = nn.Linear(cfg.rnn_units, NPY_FEATURES)
        
        self.cnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, 64 * 16 * 16)
        self.cnn_decoder = nn.Sequential(
            nn.Unflatten(1, (64, 16, 16)),
            nn.ConvTranspose2d(64, 64, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, 2, 1, 1), nn.ReLU(),
            nn.Conv2d(16, 1, 3, 1, 1), nn.Sigmoid()
        )
        
    def forward(self, img, npy):
        cnn_feat = self.cnn_encoder(img)
        _, (h_n, _) = self.rnn_encoder(npy)
        z = self.bottleneck(torch.cat((cnn_feat, h_n[-1]), dim=1))
        rnn_out, _ = self.rnn_decoder(self.rnn_decoder_fc(z).unsqueeze(1).repeat(1, NPY_SEQ_LENGTH, 1))
        return self.cnn_decoder(self.cnn_decoder_fc(z)), self.rnn_output_layer(rnn_out)

# --- Scaler í•¨ìˆ˜ ---
def get_npy_scaler(df, npy_dir):
    scaler_path = "npy_scaler.joblib"
    if os.path.exists(scaler_path):
        from joblib import load
        return load(scaler_path)
    from joblib import dump
    scaler = StandardScaler()
    sample_ids = df['video_id'].sample(min(len(df), 1500), random_state=42)
    all_npy = []
    for vid in sample_ids:
        try:
            d = np.load(os.path.join(npy_dir, f"{vid}.npy"), allow_pickle=True).item()['mouth']
            all_npy.append(np.stack([d['laplacian_mean'], d['laplacian_var'], d['light_intensity_mean'], d['light_intensity_change'], d['area']], axis=1))
        except: pass
    scaler.fit(np.concatenate(all_npy).reshape(-1, NPY_FEATURES))
    dump(scaler, scaler_path)
    return scaler

# --- â˜…â˜…â˜… í•µì‹¬: Train í•¨ìˆ˜ë¡œ ë³€ê²½ â˜…â˜…â˜… ---
def train_sweep():
    # 1. WandB ì´ˆê¸°í™” (Agentê°€ ì„¤ì •ì„ ì£¼ì…í•´ì¤Œ)
    wandb.init()
    
    # WandBê°€ ì¤€ ì„¤ì •ê°’(config)ì„ ê°€ì ¸ì˜´
    config = wandb.config
    
    seed_everything(42)
    
    # 2. ë°ì´í„° ë¡œë“œ (ì „ì—­ ë³€ìˆ˜ ê²½ë¡œ ì‚¬ìš©)
    if not os.path.exists(CSV_FILE_PATH):
        print("CSV ì—†ìŒ"); return

    df_all = pd.read_csv(CSV_FILE_PATH)
    df_train, df_val = train_test_split(df_all, test_size=0.2, random_state=42)
    scaler = get_npy_scaler(df_train, NPY_DIR)
    
    # ë°ì´í„°ì…‹ & ë¡œë” (Batch SizeëŠ” Sweep configì—ì„œ ê°€ì ¸ì˜´)
    train_dataset = MultiModalRamDataset(df_train, NPY_DIR, PNG_DIR, (IMG_HEIGHT, IMG_WIDTH), (NPY_SEQ_LENGTH, NPY_FEATURES), scaler, mode='Train')
    val_dataset = MultiModalRamDataset(df_val, NPY_DIR, PNG_DIR, (IMG_HEIGHT, IMG_WIDTH), (NPY_SEQ_LENGTH, NPY_FEATURES), scaler, mode='Val')
    
    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0)
    
    # 3. ëª¨ë¸ ìƒì„±
    model = MultiModalAutoencoder(config).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
    
    # 4. í•™ìŠµ ë£¨í”„ (Epochsë„ configì— ìˆë‹¤ë©´ config.epochs, ì—†ìœ¼ë©´ ê³ ì •ê°’)
    epochs = getattr(config, 'epochs', 15) # Sweep í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 15íšŒ ì •ë„ë¡œ ì¤„ì„ (ê¶Œì¥)
    
    print(f"ğŸš€ Sweep Start: LR={config.learning_rate}, BS={config.batch_size}, Model={config.cnn_model}")
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        
        for (img_in, npy_in), (img_t, npy_t) in train_loader:
            img_in, npy_in, img_t, npy_t = img_in.to(device), npy_in.to(device), img_t.to(device), npy_t.to(device)
            optimizer.zero_grad()
            p_out, n_out = model(img_in, npy_in)
            loss = criterion(p_out, img_t) + criterion(n_out, npy_t)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for (img_in, npy_in), (img_t, npy_t) in val_loader:
                img_in, npy_in, img_t, npy_t = img_in.to(device), npy_in.to(device), img_t.to(device), npy_t.to(device)
                p_out, n_out = model(img_in, npy_in)
                val_loss += (criterion(p_out, img_t) + criterion(n_out, npy_t)).item()
                
        avg_train = train_loss / len(train_loader)
        avg_val = val_loss / len(val_loader)
        
        # WandBì— ê¸°ë¡ (ë§¤ Epoch ë§ˆë‹¤)
        wandb.log({"epoch": epoch+1, "train_loss": avg_train, "val_loss": avg_val})
        
    print(f"âœ¨ Sweep Run Finished: Val Loss = {avg_val:.6f}")

# if __name__ == "__main__": ë¶€ë¶„ì€ ì´ì œ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
# ë³„ë„ì˜ ì‹¤í–‰ íŒŒì¼ì—ì„œ í˜¸ì¶œí•  ê²ƒì…ë‹ˆë‹¤.