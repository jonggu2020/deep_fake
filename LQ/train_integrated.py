# train_integrated.py
# (48GB RAM í™œìš©: 100íšŒ ìë™ íŠœë‹ ë²„ì „)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import numpy as np
import os
import wandb
import xgboost as xgb
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import log_loss, accuracy_score
from tqdm import tqdm

# ì„¤ì • íŒŒì¼ ì„í¬íŠ¸
from sweep_config_integrated import sweep_config 

# --- 1. ì‚¬ìš©ì ì„¤ì • (ê²½ë¡œ) ---

# âš ï¸ [í™•ì¸] ë³¸ì¸ì˜ í™˜ê²½ì— ë§ëŠ” íŒŒì¼ ê²½ë¡œì¸ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”.
CSV_FILE_PATH = "./cleaned_statistics_all_merged.csv"
NPY_DIR = "./2_npy_timeseries"

# NPY ë°ì´í„° ì„¤ì •
NPY_SEQ_LENGTH = 90
NPY_FEATURES = 5

# GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ (In-Memory Loading) ---

def load_all_data_to_ram():
    print("="*60)
    print(f"ğŸš€ [RAM ìµœì í™”] ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤...")
    print(f"   - CSV ê²½ë¡œ: {CSV_FILE_PATH}")
    print(f"   - NPY í´ë”: {NPY_DIR}")
    print("="*60)
    
    if not os.path.exists(CSV_FILE_PATH):
        print(f"âŒ [ì˜¤ë¥˜] CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CSV_FILE_PATH}")
        return None

    try:
        # 1. CSV ë¡œë“œ ë° ì „ì²˜ë¦¬
        df = pd.read_csv(CSV_FILE_PATH)
        num_cols = df.select_dtypes(include=[np.number]).columns
        df[num_cols] = df[num_cols].fillna(0)
        feat_cols = [c for c in num_cols if c not in ['label']]
        
        print(f"âœ“ CSV ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
        
        # 2. Tabular Scaler Fitting
        print("ğŸ“Š ì •í˜• ë°ì´í„°(Tabular) ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        tab_scaler = StandardScaler().fit(df[feat_cols])
        X_tab_all = tab_scaler.transform(df[feat_cols]) # (N, Features)
        
        # 3. Pseudo-labeling (XGBoost í•™ìŠµìš©)
        print("ğŸŒ² Isolation Forestë¡œ ê°€ìƒ ë¼ë²¨(Pseudo-label) ìƒì„± ì¤‘...")
        iso = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)
        pseudo_labels = (iso.fit_predict(X_tab_all) == -1).astype(int)
        print(f"   - ì •ìƒ(0): {np.sum(pseudo_labels==0)}, ì´ìƒ(1): {np.sum(pseudo_labels==1)}")
        
        # 4. NPY ë°ì´í„° ì „ëŸ‰ ë¡œë“œ
        print(f"ğŸ“¥ ì‹œê³„ì—´ ë°ì´í„°(NPY) ë¡œë“œ ì¤‘...")
        
        # ê²°ê³¼ë¥¼ ë‹´ì„ ë¹ˆ ë°°ì—´ ìƒì„± (N, 90, 5)
        X_npy_all = np.zeros((len(df), NPY_SEQ_LENGTH, NPY_FEATURES), dtype=np.float32)
        npy_samples_for_scaler = []
        
        missing_count = 0
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="NPY Loading"):
            video_id = row['video_id']
            npy_path = os.path.join(NPY_DIR, f"{video_id}.npy")
            
            if os.path.exists(npy_path):
                try:
                    d = np.load(npy_path, allow_pickle=True).item()
                    # 'mouth' íŠ¹ì§• ì¶”ì¶œ
                    m = np.stack([
                        d['mouth']['laplacian_mean'], d['mouth']['laplacian_var'],
                        d['mouth']['light_intensity_mean'], d['mouth']['light_intensity_change'],
                        d['mouth']['area']
                    ], axis=1) # (T, 5)
                    
                    # ê¸¸ì´ ë§ì¶”ê¸° (Padding/Truncating)
                    curr = m.shape[0]
                    if curr > NPY_SEQ_LENGTH:
                        m = m[:NPY_SEQ_LENGTH]
                    elif curr < NPY_SEQ_LENGTH:
                        m = np.vstack([m, np.zeros((NPY_SEQ_LENGTH - curr, NPY_FEATURES))])
                    
                    X_npy_all[idx] = m
                    
                    # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµìš© ìƒ˜í”Œë§ (ì•ìª½ 5000ê°œë§Œ)
                    if len(npy_samples_for_scaler) < 5000:
                        npy_samples_for_scaler.append(m)
                        
                except Exception:
                    missing_count += 1
            else:
                missing_count += 1
        
        if missing_count > 0:
            print(f"   âš ï¸ {missing_count}ê°œì˜ NPY íŒŒì¼ ëˆ„ë½ (0ìœ¼ë¡œ ëŒ€ì²´ë¨)")
            
        # 5. NPY Scaler Fitting & Transform
        print("ğŸ“‰ ì‹œê³„ì—´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        if npy_samples_for_scaler:
            npy_scaler = StandardScaler()
            npy_scaler.fit(np.concatenate(npy_samples_for_scaler))
            
            # ì „ì²´ ë°ì´í„° ë³€í™˜
            N, T, F = X_npy_all.shape
            X_npy_flat = X_npy_all.reshape(-1, F)
            X_npy_flat = npy_scaler.transform(X_npy_flat)
            X_npy_all = X_npy_flat.reshape(N, T, F)
        else:
            print("âš ï¸ NPY ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ìŠ¤ì¼€ì¼ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # 6. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
        indices = np.arange(len(df))
        train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=pseudo_labels, random_state=42)
        
        # ìµœì¢… ë°ì´í„° ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
        data = {
            "train": {
                "tab": X_tab_all[train_idx],
                "npy": X_npy_all[train_idx],
                "y": pseudo_labels[train_idx]
            },
            "val": {
                "tab": X_tab_all[val_idx],
                "npy": X_npy_all[val_idx],
                "y": pseudo_labels[val_idx]
            },
            "input_dim": len(feat_cols)
        }
        
        print("âœ“ ëª¨ë“  ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")
        return data

    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        return None

# --- 3. ëª¨ë¸ ì •ì˜ (Autoencoders) ---

class TabularAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128), nn.ReLU(), nn.BatchNorm1d(128),
            nn.Linear(128, latent_dim), nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128), nn.ReLU(), nn.BatchNorm1d(128),
            nn.Linear(128, input_dim)
        )
    def forward(self, x): return self.decoder(self.encoder(x))

class RNNAE(nn.Module):
    def __init__(self, rnn_type, hidden_dim, num_layers):
        super().__init__()
        self.rnn_type = rnn_type
        if rnn_type == 'LSTM':
            self.enc = nn.LSTM(NPY_FEATURES, hidden_dim, num_layers, batch_first=True)
            self.dec = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)
        else:
            self.enc = nn.GRU(NPY_FEATURES, hidden_dim, num_layers, batch_first=True)
            self.dec = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.out = nn.Linear(hidden_dim, NPY_FEATURES)
        
    def forward(self, x):
        if self.rnn_type == 'LSTM': _, (h, _) = self.enc(x)
        else: _, h = self.enc(x)
        
        # ë§ˆì§€ë§‰ Hidden stateë¥¼ ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ë°˜ë³µí•˜ì—¬ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
        h_rep = h[-1].unsqueeze(1).repeat(1, NPY_SEQ_LENGTH, 1)
        dec_out, _ = self.dec(h_rep)
        return self.out(dec_out)

# --- 4. ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ (WandB Agentê°€ í˜¸ì¶œ) ---

def train_pipeline():
    # WandB ì´ˆê¸°í™” (Sweep Controllerê°€ ì„¤ì •ì„ ì£¼ì…)
    wandb.init()
    cfg = wandb.config
    
    # --- Phase 1: XGBoost í•™ìŠµ ---
    print(f"\nğŸš€ [Sweep Run] XGBoost í•™ìŠµ: n_estimators={cfg.xgb_n_estimators}, depth={cfg.xgb_max_depth}")
    
    xgb_model = xgb.XGBClassifier(
        n_estimators=cfg.xgb_n_estimators,
        max_depth=cfg.xgb_max_depth,
        learning_rate=cfg.xgb_learning_rate,
        objective='binary:logistic',
        tree_method='hist', 
        device="cuda" if torch.cuda.is_available() else "cpu",
        random_state=42
    )
    
    # RAMì— ìˆëŠ” ë°ì´í„° ì‚¬ìš©
    xgb_model.fit(
        GLOBAL_DATA['train']['tab'], GLOBAL_DATA['train']['y'],
        eval_set=[(GLOBAL_DATA['val']['tab'], GLOBAL_DATA['val']['y'])],
        verbose=False
    )
    
    # XGBoost í‰ê°€
    xgb_preds = xgb_model.predict_proba(GLOBAL_DATA['val']['tab'])[:, 1]
    xgb_loss = log_loss(GLOBAL_DATA['val']['y'], xgb_preds)
    xgb_acc = accuracy_score(GLOBAL_DATA['val']['y'], (xgb_preds > 0.5).astype(int))
    
    print(f"   âœ… XGBoost ì™„ë£Œ | Val Loss: {xgb_loss:.4f} | Acc: {xgb_acc:.4f}")
    
    # --- Phase 2: Deep Learning (PyTorch) í•™ìŠµ ---
    print(f"ğŸš€ [Sweep Run] DL í•™ìŠµ: Tabular({cfg.tab_latent_dim}) + RNN({cfg.rnn_type}/{cfg.rnn_hidden_dim})")
    
    # TensorDataset ë³€í™˜
    train_ds = TensorDataset(
        torch.FloatTensor(GLOBAL_DATA['train']['tab']),
        torch.FloatTensor(GLOBAL_DATA['train']['npy'])
    )
    val_ds = TensorDataset(
        torch.FloatTensor(GLOBAL_DATA['val']['tab']),
        torch.FloatTensor(GLOBAL_DATA['val']['npy'])
    )
    
    train_loader = DataLoader(train_ds, batch_size=cfg.dl_batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=cfg.dl_batch_size, shuffle=False)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    tab_input_dim = GLOBAL_DATA['input_dim']
    model_tab = TabularAE(tab_input_dim, cfg.tab_latent_dim).to(device)
    model_rnn = RNNAE(cfg.rnn_type, cfg.rnn_hidden_dim, cfg.rnn_layers).to(device)
    
    optimizer = optim.Adam(
        list(model_tab.parameters()) + list(model_rnn.parameters()), 
        lr=cfg.dl_learning_rate
    )
    criterion = nn.MSELoss()
    
    # í•™ìŠµ ë£¨í”„ (15 Epoch)
    epochs = 15
    
    for epoch in range(epochs):
        model_tab.train(); model_rnn.train()
        
        for tab_x, npy_x in train_loader:
            tab_x, npy_x = tab_x.to(device), npy_x.to(device)
            
            optimizer.zero_grad()
            loss = criterion(model_tab(tab_x), tab_x) + criterion(model_rnn(npy_x), npy_x)
            loss.backward()
            optimizer.step()
            
        # Validation
        model_tab.eval(); model_rnn.eval()
        val_loss_sum = 0; val_tab_sum = 0; val_rnn_sum = 0
        
        with torch.no_grad():
            for tab_x, npy_x in val_loader:
                tab_x, npy_x = tab_x.to(device), npy_x.to(device)
                l_tab = criterion(model_tab(tab_x), tab_x)
                l_rnn = criterion(model_rnn(npy_x), npy_x)
                
                val_loss_sum += (l_tab + l_rnn).item()
                val_tab_sum += l_tab.item()
                val_rnn_sum += l_rnn.item()
                
        avg_dl_loss = val_loss_sum / len(val_loader)
        avg_tab_loss = val_tab_sum / len(val_loader)
        avg_rnn_loss = val_rnn_sum / len(val_loader)
        
        # ì¢…í•© ì ìˆ˜ (XGBoost Loss + DL Reconstruction Loss)
        global_score = avg_dl_loss + xgb_loss 
        
        wandb.log({
            "epoch": epoch + 1,
            "global_score": global_score,
            "xgb_val_loss": xgb_loss,
            "dl_total_val_loss": avg_dl_loss,
            "ae_tabular_loss": avg_tab_loss,
            "ae_rnn_loss": avg_rnn_loss
        })
        
    print(f"   ğŸ í•™ìŠµ ì¢…ë£Œ | Final Global Score: {global_score:.4f}")

# --- 5. ì‹¤í–‰ë¶€ ---

if __name__ == "__main__":
    
    # 1. ë°ì´í„° ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰)
    # ì „ì—­ ë³€ìˆ˜ GLOBAL_DATAì— ë°ì´í„°ë¥¼ ì˜¬ë ¤ë‘ê³  100ë²ˆì˜ Sweep ë™ì•ˆ ê³„ì† ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    GLOBAL_DATA = load_all_data_to_ram()
    
    if GLOBAL_DATA:
        print("\n" + "="*60)
        print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! WandB Sweep Agentë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        print("ğŸš€ ì´ 100íšŒì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
        print("="*60 + "\n")
        
        # 2. Sweep í”„ë¡œì íŠ¸ ë“±ë¡
        sweep_id = wandb.sweep(sweep_config, project="deepfake-LQ-Ensemble-RAM")
        
        # 3. Agent ì‹¤í–‰ (count=100)
        wandb.agent(sweep_id, function=train_pipeline, count=100)
    else:
        print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ë¡œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")