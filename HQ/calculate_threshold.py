# calculate_threshold.py
# (í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì •ìƒ ë°ì´í„°ì˜ ë³µì› ì˜¤ì°¨ ë¶„í¬ í™•ì¸ ë° ì„ê³„ê°’ ì„¤ì •)

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import os
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader
from types import SimpleNamespace
from joblib import load
from tqdm import tqdm

# --- 1. ì„¤ì • ë° ê²½ë¡œ (í•™ìŠµ ë•Œì™€ ë™ì¼í•´ì•¼ í•¨) ---
config = SimpleNamespace(
    batch_size = 256, # ì¶”ë¡  ë•ŒëŠ” ì»¤ë„ ìƒê´€ì—†ìŒ
    bottleneck_dim = 64,
    cnn_latent_dim = 64,
    cnn_model = "AlexNet_Mini",
    rnn_model = "LSTM",
    rnn_units = 64
)

# íŒŒì¼ ê²½ë¡œ í™•ì¸
CSV_FILE_PATH = "./FINAL_master_summary_28828.csv" 
NPY_DIR = "./FINAL_NPY_28828"
PNG_DIR = "./3_audio_spectrograms"
MODEL_PATH = "best_multimodal_ae_torch_ram.pt" # ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ëª… í™•ì¸!
SCALER_PATH = "npy_scaler.joblib"

IMG_HEIGHT, IMG_WIDTH = 128, 128
NPY_SEQ_LENGTH, NPY_FEATURES = 90, 5 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ êµ¬ì¡°ê°€ 100% ì¼ì¹˜í•´ì•¼ í•¨) ---
class MultiModalAutoencoder(nn.Module):
    def __init__(self, cfg):
        super(MultiModalAutoencoder, self).__init__()
        self.cfg = cfg
        
        # CNN Encoder
        self.cnn_encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=5, stride=2, padding=0), nn.ReLU(), nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2),
            nn.Flatten(), nn.Linear(64 * 15 * 15, cfg.cnn_latent_dim), nn.ReLU()
        )
        # RNN Encoder
        self.rnn_encoder = nn.LSTM(input_size=NPY_FEATURES, hidden_size=cfg.rnn_units, batch_first=True)
        # Bottleneck
        self.bottleneck = nn.Sequential(nn.Linear(cfg.cnn_latent_dim + cfg.rnn_units, cfg.bottleneck_dim), nn.ReLU())
        
        # RNN Decoder
        self.rnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, cfg.rnn_units)
        self.rnn_decoder = nn.LSTM(input_size=cfg.rnn_units, hidden_size=cfg.rnn_units, batch_first=True)
        self.rnn_output_layer = nn.Linear(cfg.rnn_units, NPY_FEATURES)
        
        # CNN Decoder
        self.cnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, 64 * 16 * 16)
        self.cnn_decoder = nn.Sequential(
            nn.Unflatten(1, (64, 16, 16)),
            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1), nn.ReLU(),
            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1), nn.ReLU(),
            nn.Conv2d(16, 1, kernel_size=3, padding=1), nn.Sigmoid()
        )

    def forward(self, img, npy):
        cnn_feat = self.cnn_encoder(img)
        _, (h_n, _) = self.rnn_encoder(npy)
        z = self.bottleneck(torch.cat((cnn_feat, h_n[-1]), dim=1))
        
        rnn_out, _ = self.rnn_decoder(self.rnn_decoder_fc(z).unsqueeze(1).repeat(1, NPY_SEQ_LENGTH, 1))
        return self.cnn_decoder(self.cnn_decoder_fc(z)), self.rnn_output_layer(rnn_out)

# --- 3. ë°ì´í„°ì…‹ ì •ì˜ (ì¶”ë¡ ìš©) ---
class InferenceDataset(Dataset):
    def __init__(self, df, npy_dir, png_dir, scaler):
        self.df = df.reset_index(drop=True)
        self.npy_dir = npy_dir
        self.png_dir = png_dir
        self.scaler = scaler
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        video_id = self.df.loc[index, 'video_id']
        
        # PNG
        try:
            path = os.path.join(self.png_dir, f"{video_id}.png")
            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: raise Exception
            img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT)) / 255.0
            img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0)
        except:
            img_tensor = torch.zeros((1, IMG_HEIGHT, IMG_WIDTH), dtype=torch.float32)

        # NPY
        try:
            path = os.path.join(self.npy_dir, f"{video_id}.npy")
            d = np.load(path, allow_pickle=True).item()['mouth']
            mouth = np.stack([d['laplacian_mean'], d['laplacian_var'], d['light_intensity_mean'], d['light_intensity_change'], d['area']], axis=1)
            mouth_s = self.scaler.transform(mouth.reshape(-1, NPY_FEATURES))
            
            pad = np.zeros((NPY_SEQ_LENGTH, NPY_FEATURES))
            length = min(len(mouth_s), NPY_SEQ_LENGTH)
            pad[:length, :] = mouth_s[:length, :]
            npy_tensor = torch.tensor(pad, dtype=torch.float32)
        except:
            npy_tensor = torch.zeros((NPY_SEQ_LENGTH, NPY_FEATURES), dtype=torch.float32)
            
        return img_tensor, npy_tensor, video_id

# --- 4. ë©”ì¸ ì‹¤í–‰: ì„ê³„ê°’ ê³„ì‚° ---
if __name__ == "__main__":
    print(f"ğŸš€ ì„ê³„ê°’ ê³„ì‚° ì‹œì‘ (Device: {device})")
    
    # 1. ë°ì´í„° ì¤€ë¹„ (Validation Setë§Œ ì‚¬ìš©)
    df_all = pd.read_csv(CSV_FILE_PATH)
    _, df_val = train_test_split(df_all, test_size=0.2, random_state=42)
    print(f"ğŸ“Š ê²€ì¦ ë°ì´í„° ê°œìˆ˜: {len(df_val)}ê°œ")
    
    scaler = load(SCALER_PATH)
    val_dataset = InferenceDataset(df_val, NPY_DIR, PNG_DIR, scaler)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)
    
    # 2. ëª¨ë¸ ë¡œë“œ
    model = MultiModalAutoencoder(config).to(device)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    print("âœ… í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # 3. ë³µì› ì˜¤ì°¨(Loss) ê³„ì‚°
    losses = []
    video_ids = []
    criterion = nn.MSELoss(reduction='none') # ìƒ˜í”Œë³„ Loss ê³„ì‚°ì„ ìœ„í•´ reduction='none'
    
    print("ğŸ” ë³µì› ì˜¤ì°¨ ê³„ì‚° ì¤‘...")
    with torch.no_grad():
        for img, npy, v_ids in tqdm(val_loader):
            img, npy = img.to(device), npy.to(device)
            
            # Forward
            p_out, n_out = model(img, npy)
            
            # Loss ê³„ì‚° (í‰ê· ì´ ì•„ë‹ˆë¼ ê°œë³„ ìƒ˜í”Œì˜ Loss í•©)
            # ì´ë¯¸ì§€ Loss: (B, 1, 128, 128) -> (B,)
            loss_p = torch.mean((p_out - img)**2, dim=[1, 2, 3]) 
            # NPY Loss: (B, 90, 5) -> (B,)
            loss_n = torch.mean((n_out - npy)**2, dim=[1, 2])
            
            # Total Loss (ë‹¨ìˆœ í•© ë˜ëŠ” ê°€ì¤‘ì¹˜ ì ìš©)
            total_loss = loss_p + loss_n
            
            losses.extend(total_loss.cpu().numpy())
            video_ids.extend(v_ids)
            
    # 4. í†µê³„ ë¶„ì„ ë° ì‹œê°í™”
    losses = np.array(losses)
    mean_loss = np.mean(losses)
    std_loss = np.std(losses)
    max_loss = np.max(losses)
    
    print("\n" + "="*30)
    print(f"ğŸ“Š [ì •ìƒ ë°ì´í„° ë³µì› ì˜¤ì°¨ í†µê³„]")
    print(f" - í‰ê· (Mean): {mean_loss:.4f}")
    print(f" - í‘œì¤€í¸ì°¨(Std): {std_loss:.4f}")
    print(f" - ìµœì†Œ(Min): {np.min(losses):.4f}")
    print(f" - ìµœëŒ€(Max): {max_loss:.4f}")
    print("="*30)
    
    # 5. ì¶”ì²œ ì„ê³„ê°’ ì œì•ˆ
    # ë°©ë²• 1: í‰ê·  + 2 * í‘œì¤€í¸ì°¨ (ì•½ 95% ì»¤ë²„)
    threshold_2std = mean_loss + 2 * std_loss
    # ë°©ë²• 2: í‰ê·  + 3 * í‘œì¤€í¸ì°¨ (ì•½ 99% ì»¤ë²„, ë³´ìˆ˜ì )
    threshold_3std = mean_loss + 3 * std_loss
    # ë°©ë²• 3: ìµœëŒ€ê°’ (ë°ì´í„°ê°€ ê¹¨ë—í•˜ë‹¤ë©´ ê°€ì¥ ì•ˆì „)
    threshold_max = max_loss
    
    print(f"\nğŸ’¡ [ì¶”ì²œ ì„ê³„ê°’(Threshold)]")
    print(f"1ï¸âƒ£ ëŠìŠ¨í•œ ê¸°ì¤€ (Mean + 2Ïƒ): {threshold_2std:.4f} (ì´ ê°’ ì´ìƒì´ë©´ ì˜ì‹¬)")
    print(f"2ï¸âƒ£ ì—„ê²©í•œ ê¸°ì¤€ (Mean + 3Ïƒ): {threshold_3std:.4f} (í™•ì‹¤í•œ ì´ìƒì¹˜ë§Œ íƒì§€)")
    print(f"3ï¸âƒ£ ìµœëŒ€ê°’ ê¸°ì¤€ (Max Val):    {threshold_max:.4f} (Validation ë°ì´í„° ë‚´ ëª¨ë“  ì •ìƒ ì¼€ì´ìŠ¤ í¬í•¨)")
    
    # 6. íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(10, 6))
    sns.histplot(losses, bins=50, kde=True, color='blue', label='Normal Data')
    plt.axvline(threshold_2std, color='orange', linestyle='--', label=f'Threshold (2std): {threshold_2std:.2f}')
    plt.axvline(threshold_3std, color='red', linestyle='--', label=f'Threshold (3std): {threshold_3std:.2f}')
    plt.title("Reconstruction Error Distribution (Normal Data)")
    plt.xlabel("Reconstruction Loss (MSE)")
    plt.ylabel("Count")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    save_path = "threshold_distribution.png"
    plt.savefig(save_path)
    print(f"\nğŸ“ˆ íˆìŠ¤í† ê·¸ë¨ ì €ì¥ ì™„ë£Œ: {save_path}")
    print("ì´ ê·¸ë˜í”„ë¥¼ ë³´ê³  ì ì ˆí•œ ì„ê³„ê°’ì„ ì„ íƒí•˜ì„¸ìš”.")