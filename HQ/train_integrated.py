# train_integrated_ram.py
# (48GB RAM í™œìš©: ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì—¬ ì´ˆê³ ì† í•™ìŠµ)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import numpy as np
import os
import wandb
import xgboost as xgb
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import log_loss, accuracy_score
from tqdm import tqdm

# ì„¤ì • íŒŒì¼ ì„í¬íŠ¸ (ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©)
from sweep_config_integrated import sweep_config 

# --- 1. ì‚¬ìš©ì ì„¤ì • (ê²½ë¡œ) ---

# âš ï¸ [ìˆ˜ì •í•„ìš” 1] 43,000ê°œ ì›ë³¸ CSV íŒŒì¼ ê²½ë¡œ
CSV_FILE_PATH = "./master_summary_v11_cleaned_final.csv"

# âš ï¸ [ìˆ˜ì •í•„ìš” 2] 43,000ê°œ NPY íŒŒì¼ í´ë”
NPY_DIR = "./2_npy_timeseries"

# NPY ì„¤ì •
NPY_SEQ_LENGTH = 90
NPY_FEATURES = 5

# GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ PyTorch Device: {device}")

# --- 2. ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ (In-Memory Loading) ---

def load_all_data_to_ram():
    print("="*50)
    print("ğŸš€ [RAM ìµœì í™”] ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤...")
    print("="*50)
    
    try:
        # 1. CSV ë¡œë“œ ë° ì „ì²˜ë¦¬
        df = pd.read_csv(CSV_FILE_PATH)
        num_cols = df.select_dtypes(include=[np.number]).columns
        df[num_cols] = df[num_cols].fillna(0)
        feat_cols = [c for c in num_cols if c not in ['label']]
        
        print(f"âœ“ CSV ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰")
        
        # 2. CSV Scaler Fitting
        print("ğŸ“Š Tabular Scaler í”¼íŒ… ì¤‘...")
        tab_scaler = StandardScaler().fit(df[feat_cols])
        X_tab_all = tab_scaler.transform(df[feat_cols]) # (N, 120) Numpy ë°°ì—´
        
        # 3. Pseudo-labeling (XGBoostìš©)
        print("ğŸŒ² Isolation Forestë¡œ Pseudo-label ìƒì„± ì¤‘...")
        iso = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)
        pseudo_labels = (iso.fit_predict(X_tab_all) == -1).astype(int)
        print(f"   - ì •ìƒ(0): {np.sum(pseudo_labels==0)}, ì˜ì‹¬(1): {np.sum(pseudo_labels==1)}")
        
        # 4. NPY ë°ì´í„° ì „ëŸ‰ ë¡œë“œ (í•µì‹¬ ìµœì í™”)
        print(f"ğŸ“¥ 43,000ê°œ NPY íŒŒì¼ ë¡œë“œ ì¤‘ (RAM: 48GB ì¶©ë¶„)...")
        
        # ê²°ê³¼ë¥¼ ë‹´ì„ ë¹ˆ ë°°ì—´ ìƒì„± (N, 90, 5)
        X_npy_all = np.zeros((len(df), NPY_SEQ_LENGTH, NPY_FEATURES), dtype=np.float32)
        
        # NPY Scalerë¥¼ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘
        npy_samples_for_scaler = []
        
        # íŒŒì¼ ë¡œë”© ë£¨í”„
        missing_count = 0
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="NPY ë¡œë”©"):
            video_id = row['video_id']
            npy_path = os.path.join(NPY_DIR, f"{video_id}.npy")
            
            if os.path.exists(npy_path):
                try:
                    d = np.load(npy_path, allow_pickle=True).item()
                    # 'mouth' íŠ¹ì§• ì¶”ì¶œ
                    m = np.stack([
                        d['mouth']['laplacian_mean'], d['mouth']['laplacian_var'],
                        d['mouth']['light_intensity_mean'], d['mouth']['light_intensity_change'],
                        d['mouth']['area']
                    ], axis=1) # (T, 5)
                    
                    # Pad/Truncate (ê¸¸ì´ ë§ì¶”ê¸°)
                    curr = m.shape[0]
                    if curr > NPY_SEQ_LENGTH:
                        m = m[:NPY_SEQ_LENGTH]
                    elif curr < NPY_SEQ_LENGTH:
                        m = np.vstack([m, np.zeros((NPY_SEQ_LENGTH - curr, NPY_FEATURES))])
                    
                    X_npy_all[idx] = m
                    
                    # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµìš© ìƒ˜í”Œë§ (ì²˜ìŒ 5000ê°œë§Œ ì‚¬ìš©)
                    if len(npy_samples_for_scaler) < 5000:
                        npy_samples_for_scaler.append(m)
                        
                except Exception:
                    missing_count += 1
            else:
                missing_count += 1
        
        if missing_count > 0:
            print(f"   âš ï¸ {missing_count}ê°œì˜ NPY íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ìˆì–´ 0ìœ¼ë¡œ ì±„ì› ìŠµë‹ˆë‹¤.")
            
        # 5. NPY Scaler Fitting & Transform
        print("ğŸ“‰ NPY ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        if npy_samples_for_scaler:
            # ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ…
            npy_scaler = StandardScaler()
            npy_scaler.fit(np.concatenate(npy_samples_for_scaler))
            
            # ì „ì²´ ë°ì´í„° ë³€í™˜ (Batch ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨í™”)
            # (N, 90, 5) -> (N*90, 5) -> transform -> (N, 90, 5)
            N, T, F = X_npy_all.shape
            X_npy_flat = X_npy_all.reshape(-1, F)
            X_npy_flat = npy_scaler.transform(X_npy_flat)
            X_npy_all = X_npy_flat.reshape(N, T, F)
        else:
            print("âš ï¸ NPY ë°ì´í„°ê°€ ì—†ì–´ ìŠ¤ì¼€ì¼ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # 6. í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬ (Indices)
        indices = np.arange(len(df))
        train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=pseudo_labels, random_state=42)
        
        # ìµœì¢… ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        data = {
            "train": {
                "tab": X_tab_all[train_idx],
                "npy": X_npy_all[train_idx],
                "y": pseudo_labels[train_idx]
            },
            "val": {
                "tab": X_tab_all[val_idx],
                "npy": X_npy_all[val_idx],
                "y": pseudo_labels[val_idx]
            },
            "input_dim": len(feat_cols)
        }
        
        print("âœ“ ëª¨ë“  ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")
        return data

    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        return None

# --- 3. PyTorch ëª¨ë¸ ì •ì˜ ---

class TabularAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128), nn.ReLU(), nn.BatchNorm1d(128),
            nn.Linear(128, latent_dim), nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128), nn.ReLU(), nn.BatchNorm1d(128),
            nn.Linear(128, input_dim)
        )
    def forward(self, x): return self.decoder(self.encoder(x))

class RNNAE(nn.Module):
    def __init__(self, rnn_type, hidden_dim, num_layers):
        super().__init__()
        self.rnn_type = rnn_type
        if rnn_type == 'LSTM':
            self.enc = nn.LSTM(NPY_FEATURES, hidden_dim, num_layers, batch_first=True)
            self.dec = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)
        else:
            self.enc = nn.GRU(NPY_FEATURES, hidden_dim, num_layers, batch_first=True)
            self.dec = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.out = nn.Linear(hidden_dim, NPY_FEATURES)
        
    def forward(self, x):
        if self.rnn_type == 'LSTM': _, (h, _) = self.enc(x)
        else: _, h = self.enc(x)
        h_rep = h[-1].unsqueeze(1).repeat(1, NPY_SEQ_LENGTH, 1)
        dec_out, _ = self.dec(h_rep)
        return self.out(dec_out)

# --- 4. ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ (WandB Agent) ---

def train_pipeline():
    wandb.init()
    cfg = wandb.config
    
    # --- Phase 1: XGBoost í•™ìŠµ ---
    print("\nğŸš€ [Phase 1] XGBoost í•™ìŠµ (RAM)")
    
    xgb_model = xgb.XGBClassifier(
        n_estimators=cfg.xgb_n_estimators,
        max_depth=cfg.xgb_max_depth,
        learning_rate=cfg.xgb_learning_rate,
        objective='binary:logistic',
        tree_method='hist', 
        device="cuda" if torch.cuda.is_available() else "cpu",
        random_state=42
    )
    
    # ë©”ëª¨ë¦¬ì— ìˆëŠ” ë°ì´í„° ë°”ë¡œ ì‚¬ìš©
    xgb_model.fit(
        GLOBAL_DATA['train']['tab'], GLOBAL_DATA['train']['y'],
        eval_set=[(GLOBAL_DATA['val']['tab'], GLOBAL_DATA['val']['y'])],
        verbose=False
    )
    
    # í‰ê°€
    xgb_preds = xgb_model.predict_proba(GLOBAL_DATA['val']['tab'])[:, 1]
    xgb_loss = log_loss(GLOBAL_DATA['val']['y'], xgb_preds)
    xgb_acc = accuracy_score(GLOBAL_DATA['val']['y'], (xgb_preds > 0.5).astype(int))
    
    print(f"   âœ… XGBoost ì™„ë£Œ | Val Loss: {xgb_loss:.4f} | Acc: {xgb_acc:.4f}")
    
    # --- Phase 2: PyTorch í•™ìŠµ ---
    print("\nğŸš€ [Phase 2] PyTorch í•™ìŠµ (RAM)")
    
    # TensorDatasetìœ¼ë¡œ ë³€í™˜ (ì´ˆê³ ì†)
    # (ë©”ëª¨ë¦¬ì— ìˆëŠ” numpy ë°°ì—´ì„ ê·¸ëŒ€ë¡œ Tensorë¡œ ë³€í™˜)
    train_ds = TensorDataset(
        torch.FloatTensor(GLOBAL_DATA['train']['tab']),
        torch.FloatTensor(GLOBAL_DATA['train']['npy'])
    )
    val_ds = TensorDataset(
        torch.FloatTensor(GLOBAL_DATA['val']['tab']),
        torch.FloatTensor(GLOBAL_DATA['val']['npy'])
    )
    
    # DataLoader (num_workers=0 ê¶Œì¥: ì´ë¯¸ ë©”ëª¨ë¦¬ì— ìˆì–´ì„œ ë©€í‹°í”„ë¡œì„¸ì‹± ë¶ˆí•„ìš”)
    train_loader = DataLoader(train_ds, batch_size=cfg.dl_batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=cfg.dl_batch_size, shuffle=False)
    
    # ëª¨ë¸ ì„¤ì •
    tab_input_dim = GLOBAL_DATA['input_dim']
    model_tab = TabularAE(tab_input_dim, cfg.tab_latent_dim).to(device)
    model_rnn = RNNAE(cfg.rnn_type, cfg.rnn_hidden_dim, cfg.rnn_layers).to(device)
    
    optimizer = optim.Adam(list(model_tab.parameters()) + list(model_rnn.parameters()), lr=cfg.dl_learning_rate)
    criterion = nn.MSELoss()
    
    # í•™ìŠµ ë£¨í”„
    epochs = 15
    
    for epoch in range(epochs):
        model_tab.train(); model_rnn.train()
        
        for tab_x, npy_x in train_loader:
            tab_x, npy_x = tab_x.to(device), npy_x.to(device)
            
            optimizer.zero_grad()
            loss = criterion(model_tab(tab_x), tab_x) + criterion(model_rnn(npy_x), npy_x)
            loss.backward()
            optimizer.step()
            
        # Validation
        model_tab.eval(); model_rnn.eval()
        val_loss_sum = 0; val_tab_sum = 0; val_rnn_sum = 0
        
        with torch.no_grad():
            for tab_x, npy_x in val_loader:
                tab_x, npy_x = tab_x.to(device), npy_x.to(device)
                l_tab = criterion(model_tab(tab_x), tab_x)
                l_rnn = criterion(model_rnn(npy_x), npy_x)
                
                val_loss_sum += (l_tab + l_rnn).item()
                val_tab_sum += l_tab.item()
                val_rnn_sum += l_rnn.item()
                
        avg_dl_loss = val_loss_sum / len(val_loader)
        avg_tab_loss = val_tab_sum / len(val_loader)
        avg_rnn_loss = val_rnn_sum / len(val_loader)
        
        # Global Score
        global_score = avg_dl_loss + xgb_loss 
        
        wandb.log({
            "epoch": epoch + 1,
            "global_score": global_score,
            "xgb_val_loss": xgb_loss,
            "dl_total_val_loss": avg_dl_loss,
            "ae_tabular_loss": avg_tab_loss,
            "ae_rnn_loss": avg_rnn_loss
        })
        
        print(f"   Epoch {epoch+1} | Global: {global_score:.4f} (XGB: {xgb_loss:.4f} + DL: {avg_dl_loss:.4f})")

# --- 5. ì‹¤í–‰ ---

if __name__ == "__main__":
    
    # [ì¤‘ìš”] ì „ì—­ ë³€ìˆ˜ì— ë°ì´í„° ë¡œë“œ (Sweep ì‹¤í–‰ ì‹œ ì¬ë¡œë”© ë°©ì§€)
    # 48GB RAMì´ ìˆìœ¼ë¯€ë¡œ ì „ì—­ ë³€ìˆ˜ì— í•œ ë²ˆë§Œ ì˜¬ë ¤ë‘ê³  ê³„ì† ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    GLOBAL_DATA = load_all_data_to_ram()
    
    if GLOBAL_DATA:
        print("\nâœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ. WandB Agentë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        sweep_id = wandb.sweep(sweep_config, project="deepfake-Integrated-Ensemble-RAM")
        wandb.agent(sweep_id, function=train_pipeline, count=15)