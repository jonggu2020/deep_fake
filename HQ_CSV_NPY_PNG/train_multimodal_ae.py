# train_multimodal_ae.py
# (ë¹„ì •ìƒ íƒì§€ë¥¼ ìœ„í•œ ë©€í‹°ëª¨ë‹¬ ì˜¤í† ì¸ì½”ë” í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸)

import wandb
import pandas as pd
import numpy as np
import os
import cv2
import time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

# TensorFlow ë° Keras ë¼ì´ë¸ŒëŸ¬ë¦¬
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, LSTM, GRU, Conv2D, MaxPooling2D, Flatten, 
    Dropout, BatchNormalization, concatenate, Reshape,
    Conv2DTranspose, RepeatVector, TimeDistributed
)
from tensorflow.keras.utils import Sequence
from tensorflow.keras.callbacks import EarlyStopping
from wandb.integration.keras import WandbMetricsLogger # Wandb 2.0+ ìµœì‹  ì½œë°±

# --- 1. WandB Sweep ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° ---
from sweep_config_ae import sweep_config 

# --- 2. ê³ ì • ê²½ë¡œ ë° ì„¤ì • ---
# (ì´ì „ ìŠ¤í¬ë¦½íŠ¸ v9ì˜ ì¶œë ¥ ê²½ë¡œì™€ ì¼ì¹˜í•´ì•¼ í•¨)

# âš ï¸ [ê²½ë¡œ í™•ì¸ 1] ìµœì¢… CSV íŒŒì¼ (28,828ê°œ ID)
CSV_FILE_PATH = "./FINAL_master_summary_28828.csv" 

# âš ï¸ [ê²½ë¡œ í™•ì¸ 2] ìµœì¢… NPY í´ë” (28,828ê°œ íŒŒì¼)
NPY_DIR = "./FINAL_NPY_28828"

# âš ï¸ [ê²½ë¡œ í™•ì¸ 3] ìµœì¢… PNG í´ë” (28,828ê°œ íŒŒì¼)
PNG_DIR = "./3_audio_spectrograms"

# ëª¨ë¸ ì…ë ¥ í¬ê¸° ì„¤ì •
IMG_HEIGHT = 128
IMG_WIDTH = 128  # PNG ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì˜ í¬ê¸°
NPY_SEQ_LENGTH = 90 # NPY íŒŒì¼ì˜ í”„ë ˆì„ ìˆ˜ (ê°€ì •)
NPY_FEATURES = 5    # NPYì—ì„œ 'mouth' ê´€ë ¨ 5ê°œ íŠ¹ì§• (lap_mean, lap_var, light_mean, light_change, area)

# --- 3. ì»¤ìŠ¤í…€ ë°ì´í„° ìƒì„±ê¸° (Autoencoderìš©) ---
# [ìˆ˜ì •ë¨] ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ ë° íŒ¨ë”©/ì ˆì‚­ ê¸°ëŠ¥ ì¶”ê°€
class MultiModalDataGenerator(Sequence):
    def __init__(self, df, npy_dir, png_dir, batch_size,
                 img_dims, npy_dims, scaler, is_train=True):
        self.df = df.copy()
        self.npy_dir = npy_dir
        self.png_dir = png_dir
        self.batch_size = batch_size
        self.img_height, self.img_width = img_dims
        self.seq_len, self.n_features = npy_dims
        self.scaler = scaler 
        self.is_train = is_train
        
        self.ids = self.df['video_id'].values
        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.df) / self.batch_size))

    def __getitem__(self, index):
        # 'index'ë²ˆì§¸ ë°°ì¹˜ë¥¼ ìƒì„±
        start_idx = index * self.batch_size
        end_idx = (index + 1) * self.batch_size
        batch_ids = self.ids[start_idx:end_idx]
        
        X_img = np.empty((self.batch_size, self.img_height, self.img_width, 1))
        X_npy = np.empty((self.batch_size, self.seq_len, self.n_features))
        
        for i, video_id in enumerate(batch_ids):
            try:
                # 1. PNG ë¡œë“œ ë° ì „ì²˜ë¦¬
                png_path = os.path.join(self.png_dir, f"{video_id}.png")
                img = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)
                img = cv2.resize(img, (self.img_width, self.img_height))
                img_normalized = img / 255.0 
                X_img[i,] = np.expand_dims(img_normalized, axis=-1)
                
                # 2. NPY ë¡œë“œ ë° ì „ì²˜ë¦¬
                npy_path = os.path.join(self.npy_dir, f"{video_id}.npy")
                data = np.load(npy_path, allow_pickle=True).item()
                
                mouth_data = np.stack([
                    data['mouth']['laplacian_mean'],
                    data['mouth']['laplacian_var'],
                    data['mouth']['light_intensity_mean'],
                    data['mouth']['light_intensity_change'],
                    data['mouth']['area']
                ], axis=1) 
                
                # [ìˆ˜ì •ë¨] 1. ì›ë³¸ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í¬ê¸°ê°€ ì–´ë–»ë“ )
                mouth_data_scaled = self.scaler.transform(mouth_data.reshape(-1, self.n_features))
                mouth_data_scaled = mouth_data_scaled.reshape(-1, self.n_features) # (N, 5)

                # [ìˆ˜ì •ë¨] 2. ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¥¼ (90, 5)ë¡œ íŒ¨ë”©/ì ˆì‚­
                current_len = mouth_data_scaled.shape[0] # (ì˜ˆ: 89 ë˜ëŠ” 360)
                target_len = self.seq_len                # (90)
                
                # (90, 5) í¬ê¸°ì˜ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ ìƒì„±
                padded_data_scaled = np.zeros((target_len, self.n_features))
                
                if current_len > target_len: # ì ˆì‚­ (ì˜ˆ: 360 -> 90)
                    padded_data_scaled = mouth_data_scaled[:target_len, :]
                else: # íŒ¨ë”© (ì˜ˆ: 89 -> 90)
                    # 89 í”„ë ˆì„ë§Œ ë³µì‚¬ (ë§ˆì§€ë§‰ 1 í”„ë ˆì„ì€ 0ìœ¼ë¡œ ë‚¨ìŒ)
                    padded_data_scaled[:current_len, :] = mouth_data_scaled

                X_npy[i,] = padded_data_scaled
                
            except Exception as e:
                print(f"\n[ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜] ID: {video_id} / ì˜¤ë¥˜: {e}")
                X_img[i,] = 0
                X_npy[i,] = 0

        # [ìˆ˜ì •ë¨] X(ì…ë ¥)ë¥¼ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
        # (Y(ì¶œë ¥)ëŠ” ì›ë˜ ë”•ì…”ë„ˆë¦¬ì˜€ìŒ)
        X_inputs = {'png_input': X_img, 'npy_input': X_npy}
        Y_outputs = {'png_output': X_img, 'npy_output': X_npy}
        
        return X_inputs, Y_outputs

    def on_epoch_end(self):
        if self.is_train:
            np.random.shuffle(self.ids)

# --- 4. NPY ë°ì´í„° ìŠ¤ì¼€ì¼ëŸ¬ ì¤€ë¹„ ---
def get_npy_scaler(df, npy_dir):
    """'í•™ìŠµ ë°ì´í„°'ì˜ NPY í†µê³„ì¹˜(mean, std)ë¥¼ ê³„ì‚° (ë‹¨ 1íšŒ ì‹¤í–‰)"""
    print("\n[ì „ì²˜ë¦¬] NPY ë°ì´í„° ìŠ¤ì¼€ì¼ëŸ¬(StandardScaler) í”¼íŒ… ì‹œì‘...")
    
    scaler_path = "npy_scaler.joblib"
    
    # (ì‹œê°„ ì ˆì•½) ì´ë¯¸ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
    if os.path.exists(scaler_path):
        from joblib import load
        scaler = load(scaler_path)
        print("âœ“ ì €ì¥ëœ NPY ìŠ¤ì¼€ì¼ëŸ¬(npy_scaler.joblib)ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return scaler
        
    print("(í•™ìŠµ ë°ì´í„°ì˜ NPY íŒŒì¼ì„ ì½ì–´ í†µê³„ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...)")
    from joblib import dump
    scaler = StandardScaler()
    sample_ids = df['video_id'].sample(min(len(df), 1500), random_state=42)
    all_npy_data = []
    
    for video_id in tqdm(sample_ids, desc="NPY ìƒ˜í”Œ ì½ëŠ” ì¤‘"):
        try:
            npy_path = os.path.join(NPY_DIR, f"{video_id}.npy")
            data = np.load(npy_path, allow_pickle=True).item()
            
            mouth_data = np.stack([
                data['mouth']['laplacian_mean'],
                data['mouth']['laplacian_var'],
                data['mouth']['light_intensity_mean'],
                data['mouth']['light_intensity_change'],
                data['mouth']['area']
            ], axis=1)
            all_npy_data.append(mouth_data)
        except Exception:
            pass
            
    combined_data = np.concatenate(all_npy_data).reshape(-1, NPY_FEATURES)
    scaler.fit(combined_data)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    dump(scaler, scaler_path) 
    print(f"âœ“ NPY ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ… ì™„ë£Œ ë° '{scaler_path}'ì— ì €ì¥.")
    return scaler

# --- 5. ëª¨ë¸ ë¹Œë” (ì¸ì½”ë”/ë””ì½”ë”) ---

# 5-1. CNN ì¸ì½”ë” ë¸Œëœì¹˜
def build_cnn_encoder(config, img_input):
    model_type = config.cnn_model
    latent_dim = config.cnn_latent_dim
    
    if model_type == 'LeNet':
        x = Conv2D(32, (3, 3), activation='relu', padding='same')(img_input)
        x = MaxPooling2D((2, 2))(x)
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = MaxPooling2D((2, 2))(x)
        x = Flatten()(x)
        x = Dense(latent_dim, activation='relu')(x)
        return x

    elif model_type == 'AlexNet_Mini':
        x = Conv2D(32, (5, 5), strides=(2,2), activation='relu')(img_input)
        x = MaxPooling2D((2, 2))(x)
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = MaxPooling2D((2, 2))(x)
        x = Flatten()(x)
        x = Dense(latent_dim, activation='relu')(x)
        return x

    elif model_type == 'VGG_Mini':
        x = Conv2D(32, (3, 3), activation='relu', padding='same')(img_input)
        x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
        x = MaxPooling2D((2, 2))(x) # 64x64
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
        x = MaxPooling2D((2, 2))(x) # 32x32
        x = Flatten()(x)
        x = Dense(latent_dim, activation='relu')(x)
        return x

# 5-2. RNN ì¸ì½”ë” ë¸Œëœì¹˜
def build_rnn_encoder(config, npy_input):
    model_type = config.rnn_model
    units = config.rnn_units
    
    if model_type == 'LSTM':
        x = LSTM(units, return_sequences=False)(npy_input) 
        return x
    elif model_type == 'GRU':
        x = GRU(units, return_sequences=False)(npy_input)
        return x

# 5-3. ë””ì½”ë” (ë³µì›)
def build_decoders(bottleneck_vector, config):
    
    # --- RNN ë””ì½”ë” (NPY ë³µì›) ---
    # (Bottleneck -> 90, 5)
    rnn_units = config.rnn_units
    
    # ë³‘ëª© ë²¡í„°ë¥¼ RNN ìœ ë‹› ìˆ˜ë§Œí¼ í™•ì¥
    x_rnn = Dense(rnn_units, activation='relu')(bottleneck_vector)
    # (90, rnn_units) í˜•íƒœë¡œ NPY ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ë³µì œ
    x_rnn = RepeatVector(NPY_SEQ_LENGTH)(x_rnn) 
    
    if config.rnn_model == 'LSTM':
        x_rnn = LSTM(rnn_units, return_sequences=True)(x_rnn)
    else:
        x_rnn = GRU(rnn_units, return_sequences=True)(x_rnn)
        
    # (90, 5) í˜•íƒœë¡œ ë³µì› (StandardScalerë¡œ ìŠ¤ì¼€ì¼ë§ëœ ê°’)
    npy_decoded = TimeDistributed(Dense(NPY_FEATURES, activation='linear'), name='npy_output')(x_rnn)
    
    
    # --- CNN ë””ì½”ë” (PNG ë³µì›) ---
    # (Bottleneck -> 128, 128, 1)
    # ë””ì½”ë”ê°€ ì‹œì‘í•  ì ì ˆí•œ 3D í˜•íƒœ(ì˜ˆ: 16x16x64)ë¡œ Dense í™•ì¥
    start_shape = (16, 16, 64)
    x_cnn = Dense(start_shape[0] * start_shape[1] * start_shape[2], activation='relu')(bottleneck_vector)
    x_cnn = Reshape(start_shape)(x_cnn)
    
    # 16x16 -> 32x32
    x_cnn = Conv2DTranspose(64, (3, 3), strides=2, activation='relu', padding='same')(x_cnn)
    # 32x32 -> 64x64
    x_cnn = Conv2DTranspose(32, (3, 3), strides=2, activation='relu', padding='same')(x_cnn)
    # 64x64 -> 128x128
    x_cnn = Conv2DTranspose(16, (3, 3), strides=2, activation='relu', padding='same')(x_cnn)
    
    # (128, 128, 1) í˜•íƒœë¡œ ë³µì› (0~1 ì‚¬ì´ì˜ ê°’)
    png_decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='png_output')(x_cnn)
    
    return png_decoded, npy_decoded

# 5-4. ì˜¤í† ì¸ì½”ë” ëª¨ë¸ ê²°í•©
def build_autoencoder(config):
    
    # --- ì…ë ¥ ---
    img_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 1), name='png_input')
    npy_input = Input(shape=(NPY_SEQ_LENGTH, NPY_FEATURES), name='npy_input')
    
    # --- 1. ì¸ì½”ë” ---
    cnn_encoded = build_cnn_encoder(config, img_input)
    rnn_encoded = build_rnn_encoder(config, npy_input)
    
    # --- 2. ê²°í•© ë° ë³‘ëª© ---
    combined = concatenate([cnn_encoded, rnn_encoded])
    bottleneck = Dense(config.bottleneck_dim, activation='relu', name='bottleneck')(combined)
    
    # --- 3. ë””ì½”ë” ---
    png_decoded, npy_decoded = build_decoders(bottleneck, config)
    
    # --- ëª¨ë¸ ìƒì„± ---
    model = Model(inputs=[img_input, npy_input], outputs=[png_decoded, npy_decoded])
    
    # --- ì»´íŒŒì¼ ---
    optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
    
    model.compile(
        loss={
            'png_output': 'mse',  # PNG ë³µì› ì˜¤ë¥˜
            'npy_output': 'mse'   # NPY ë³µì› ì˜¤ë¥˜
        },
        loss_weights={
            'png_output': 1.0,  # PNG ì˜¤ë¥˜ ê°€ì¤‘ì¹˜
            'npy_output': 1.0   # NPY ì˜¤ë¥˜ ê°€ì¤‘ì¹˜ (ë‘ ì†ì‹¤ì„ 1:1ë¡œ ë°˜ì˜)
        },
        optimizer=optimizer
    )
    
    return model

# --- 6. WandB Sweepì„ ìœ„í•œ ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ ---

def train():
    """WandB Agentê°€ í˜¸ì¶œí•  ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    
    # 1. WandB ì´ˆê¸°í™” (Sweepì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°›ì•„ì˜´)
    wandb.init()
    config = wandb.config # í˜„ì¬ Sweepì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°

    print("\n" + "="*50)
    print(f"Sweep ì‹œì‘: {wandb.run.name}")
    print("í˜„ì¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    print(config)
    print("="*50)
    
    # 2. ë°ì´í„° ì¤€ë¹„
    try:
        df_all = pd.read_csv(CSV_FILE_PATH)
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 3. í•™ìŠµ / ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (8:2)
    df_train, df_val = train_test_split(
        df_all, 
        test_size=0.2, # 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ
        random_state=42
    )
    
    # 4. NPY ìŠ¤ì¼€ì¼ëŸ¬ ì¤€ë¹„ (í•™ìŠµ ë°ì´í„°(df_train)ë¡œë§Œ í”¼íŒ…)
    scaler = get_npy_scaler(df_train, NPY_DIR)
    
    # 5. ë°ì´í„° ìƒì„±ê¸°(Generator) ì¤€ë¹„
    train_gen = MultiModalDataGenerator(
        df_train, NPY_DIR, PNG_DIR,
        batch_size=config.batch_size,
        img_dims=(IMG_HEIGHT, IMG_WIDTH),
        npy_dims=(NPY_SEQ_LENGTH, NPY_FEATURES),
        scaler=scaler,
        is_train=True
    )
    val_gen = MultiModalDataGenerator(
        df_val, NPY_DIR, PNG_DIR,
        batch_size=config.batch_size,
        img_dims=(IMG_HEIGHT, IMG_WIDTH),
        npy_dims=(NPY_SEQ_LENGTH, NPY_FEATURES),
        scaler=scaler,
        is_train=False
    )
    
    # 6. ëª¨ë¸ êµ¬ì¶•
    model = build_autoencoder(config)
    model.summary() # ëª¨ë¸ êµ¬ì¡° ì¶œë ¥

    # 7. ì½œë°± ì„¤ì • (ì¡°ê¸° ì¢…ë£Œ ë° WandB ë¡œê¹…)
    callbacks = [
        # 5 ì—í¬í¬ ë™ì•ˆ val_loss(ì´ ë³µì› ì˜¤ë¥˜)ê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        WandbMetricsLogger(log_freq='epoch') 
    ]
    
    # 8. ëª¨ë¸ í•™ìŠµ
    model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=30, 
        callbacks=callbacks
        # workers ë° use_multiprocessing ì¸ìˆ˜ëŠ” ìµœì‹  Kerasì—ì„œ ì œê±°ë¨
    )
    print(f"Sweep ì¢…ë£Œ: {wandb.run.name}")
    wandb.finish()


# --- 7. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    
    # [í•„ìˆ˜] 1. WandB ë¡œê·¸ì¸ (í„°ë¯¸ë„ì—ì„œ 'wandb login'ì„ ë¯¸ë¦¬ ì‹¤í–‰í•´ë„ ë¨)
    # wandb.login() 
    
    print("--- ë©€í‹°ëª¨ë‹¬(AE) ë”¥í˜ì´í¬ íƒì§€ ëª¨ë¸ (ë¹„ì •ìƒ íƒì§€) ---")
    print("--- WandB í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹(Sweep) ì‹œì‘ ---")
    
    # [í•„ìˆ˜] 2. CSV, NPY, PNG ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
    if not os.path.exists(CSV_FILE_PATH):
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: '{CSV_FILE_PATH}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    elif not os.path.exists(NPY_DIR) or not os.path.exists(PNG_DIR):
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: NPY ë˜ëŠ” PNG í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # 3. WandB Sweep ìƒì„±
        sweep_id = wandb.sweep(sweep_config, project="deepfake-multimodal-AE")

        print(f"\nâœ“ WandB Sweep ìƒì„± ì™„ë£Œ (ID: {sweep_id})")
        
        # 4. WandB ì—ì´ì „íŠ¸ ì‹¤í–‰ (count=10: ì´ 10ê°€ì§€ ì¡°í•©ì„ í…ŒìŠ¤íŠ¸)
        print("WandB ì—ì´ì „íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (ì´ 10íšŒ ì‹¤í–‰)...")
        wandb.agent(sweep_id, function=train, count=10)

        print("\nğŸ‰ ëª¨ë“  Sweep ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")