import sys
import os
import io
import cv2
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import xgboost as xgb
import librosa
import librosa.display
import whisper
import dlib
import joblib
import matplotlib.pyplot as plt
import traceback
from cv2 import dnn_superres  # [í•µì‹¬] ì™¸ë¶€ ì´ˆí•´ìƒë„(Super Resolution) ë¼ì´ë¸ŒëŸ¬ë¦¬
from types import SimpleNamespace
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout, 
                             QPushButton, QLabel, QFileDialog, QLineEdit, QProgressBar, 
                             QCheckBox, QTextEdit, QGroupBox, QMessageBox, QTabWidget)
from PyQt5.QtCore import Qt, QThread, pyqtSignal
from PyQt5.QtGui import QFont

# ============================================================
# 1. ì„¤ì • ë° ìƒìˆ˜ (Configuration)
# ============================================================
DLIB_PATH = "shape_predictor_68_face_landmarks.dat"
WHISPER_SIZE = "base"
EDSR_MODEL_PATH = "./models/EDSR_x4.pb"  # [í•„ìˆ˜] ë‹¤ìš´ë¡œë“œ ë°›ì€ SR ëª¨ë¸ ê²½ë¡œ

MODEL_PATHS = {
    'HQ': {
        'xgb': './models/HQ/xgb_model.json',         
        'tab_ae': './models/HQ/tabular_ae.pth', 
        'rnn_ae': './models/HQ/rnn_ae.pth', 
        'multi_ae': './models/HQ/best_multimodal_ae_torch_ram.pt', 
        'tab_scaler': './models/HQ/tab_scaler.joblib', 
        'npy_scaler': './models/HQ/npy_scaler.joblib' 
    }
}

# [ì¤‘ìš”] HQ ëª¨ë¸ ê¸°ì¤€ ì„ê³„ê°’
THRESHOLDS = {
    'tab': {'loose': 0.03, 'strict': 0.05, 'max': 0.15},
    'rnn': {'loose': 7.0, 'strict': 10.0, 'max': 15.0},
    'multi': {'loose': 10.0, 'strict': 20.0, 'max': 30.0}
}

# ëª¨ë¸ ê°€ì¤‘ì¹˜
WEIGHTS = {'xgb': 0.1, 'rnn': 0.4, 'multi': 0.5}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

FACIAL_LANDMARKS = {
    "left_eye": list(range(36, 42)),
    "right_eye": list(range(42, 48)),
    "nose": list(range(27, 36)),
    "mouth": list(range(48, 68)),
    "jawline": list(range(0, 17)),
    "full_face": list(range(0, 68))
}

# ============================================================
# 2. í—¬í¼ í•¨ìˆ˜ (Feature Extraction Logic)
# ============================================================
def get_region_bounding_box(shape, landmark_indices):
    points = [(shape.part(i).x, shape.part(i).y) for i in landmark_indices]
    xs, ys = zip(*points)
    return (min(xs), min(ys), max(xs), max(ys))

def calculate_region_features(gray_frame, shape, region_name, landmark_indices, prev_region_mean=None):
    try:
        x_min, y_min, x_max, y_max = get_region_bounding_box(shape, landmark_indices)
        h, w = gray_frame.shape
        x_min, y_min = max(0, x_min), max(0, y_min)
        x_max, y_max = min(w, x_max), min(h, y_max)
        
        region_crop = gray_frame[y_min:y_max, x_min:x_max]
        
        if region_crop.size == 0:
            return None
        
        laplacian = cv2.Laplacian(region_crop, cv2.CV_64F)
        laplacian_mean = np.abs(laplacian).mean()
        laplacian_var = laplacian.var()
        light_intensity_mean = region_crop.mean()
        
        light_intensity_change = 0.0
        if prev_region_mean is not None:
            light_intensity_change = light_intensity_mean - prev_region_mean
        
        region_area = (x_max - x_min) * (y_max - y_min)
        
        return {
            'laplacian_mean': laplacian_mean,
            'laplacian_var': laplacian_var,
            'light_intensity_mean': light_intensity_mean,
            'light_intensity_change': light_intensity_change,
            'region_area': region_area
        }
    except:
        return None

# ============================================================
# 3. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
# ============================================================
class TabularAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128), nn.ReLU(), nn.BatchNorm1d(128),
            nn.Linear(128, latent_dim), nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128), nn.ReLU(), nn.BatchNorm1d(128),
            nn.Linear(128, input_dim)
        )
    def forward(self, x): return self.decoder(self.encoder(x))

class RNNAE(nn.Module):
    def __init__(self, rnn_type, hidden_dim, num_layers, input_dim=5):
        super().__init__()
        self.enc = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True) if rnn_type == 'LSTM' else nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)
        self.dec = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True) if rnn_type == 'LSTM' else nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.out = nn.Linear(hidden_dim, input_dim)
        self.rnn_type = rnn_type
        
    def forward(self, x):
        if self.rnn_type == 'LSTM':
             _, (h, _) = self.enc(x)
        else:
             _, h = self.enc(x)
        
        h_rep = h[-1].unsqueeze(1).repeat(1, 90, 1)
        dec_out, _ = self.dec(h_rep)
        return self.out(dec_out)

class MultiModalAutoencoder(nn.Module):
    def __init__(self, cfg):
        super(MultiModalAutoencoder, self).__init__()
        self.cfg = cfg
        self.cnn_encoder = nn.Sequential(
            nn.Conv2d(1, 32, 5, 2, 0), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Flatten(), nn.Linear(64 * 15 * 15, cfg.cnn_latent_dim), nn.ReLU()
        )
        self.cnn_decoder = nn.Sequential(
            nn.Unflatten(1, (64, 16, 16)),
            nn.ConvTranspose2d(64, 64, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, 2, 1, 1), nn.ReLU(),
            nn.Conv2d(16, 1, 3, 1, 1), nn.Sigmoid()
        )
        if cfg.rnn_model == 'LSTM':
            self.rnn_encoder = nn.LSTM(5, cfg.rnn_units, batch_first=True)
            self.rnn_decoder = nn.LSTM(cfg.rnn_units, cfg.rnn_units, batch_first=True)
        else:
            self.rnn_encoder = nn.GRU(5, cfg.rnn_units, batch_first=True)
            self.rnn_decoder = nn.GRU(cfg.rnn_units, cfg.rnn_units, batch_first=True)

        self.bottleneck = nn.Sequential(nn.Linear(cfg.cnn_latent_dim + cfg.rnn_units, cfg.bottleneck_dim), nn.ReLU())
        self.rnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, cfg.rnn_units)
        self.rnn_output_layer = nn.Linear(cfg.rnn_units, 5)
        self.cnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, 64 * 16 * 16)

    def forward(self, img, npy):
        cnn_feat = self.cnn_encoder(img)
        if self.cfg.rnn_model == 'LSTM':
             _, (h_n, _) = self.rnn_encoder(npy)
        else:
             _, h_n = self.rnn_encoder(npy)
             
        z = self.bottleneck(torch.cat((cnn_feat, h_n[-1]), dim=1))
        rnn_out, _ = self.rnn_decoder(self.rnn_decoder_fc(z).unsqueeze(1).repeat(1, 90, 1))
        return self.cnn_decoder(self.cnn_decoder_fc(z)), self.rnn_output_layer(rnn_out)

# ============================================================
# 4. ë¶„ì„ ì‘ì—… ìŠ¤ë ˆë“œ (3ê°€ì§€ ë²„ì „ ë¹„êµ ëª¨ë“œ)
# ============================================================
class AnalysisThread(QThread):
    log_signal = pyqtSignal(str)
    progress_signal = pyqtSignal(int)
    result_signal = pyqtSignal(dict)
    error_signal = pyqtSignal(str)

    def __init__(self, video_path, start_time, end_time, use_audio):
        super().__init__()
        self.video_path = video_path
        self.start_time = start_time
        self.end_time = end_time
        self.use_audio = use_audio
        self.sr_model = None

    def run(self):
        try:
            self.log_signal.emit("ğŸš€ ë¶„ì„ ì‹œì‘ (HQ ëª¨ë¸)")
            self.log_signal.emit(f"â±ï¸ ë¶„ì„ êµ¬ê°„: {self.start_time}ì´ˆ ~ {self.end_time}ì´ˆ")
            
            # 1. ì˜ìƒ í™”ì§ˆ í™•ì¸
            cap = cv2.VideoCapture(self.video_path)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            cap.release()
            
            is_low_quality = (width < 1280 and height < 720)
            
            if is_low_quality:
                self.log_signal.emit(f"ğŸ“Š ì €í™”ì§ˆ ì˜ìƒ ê°ì§€ ({width}x{height})")
                self.log_signal.emit("ğŸ”¬ HQ ëª¨ë¸ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                
                # ì›ë³¸ ì €í™”ì§ˆë§Œ ë¶„ì„
                results = {}
                
                self.log_signal.emit("\n" + "="*50)
                self.log_signal.emit("ğŸ“¹ ì›ë³¸ ì €í™”ì§ˆ ì˜ìƒ ë¶„ì„ ì¤‘...")
                self.log_signal.emit("="*50)
                results['original'] = self.analyze_video(preprocess_mode='none')
                results['denoised'] = None
                results['upscaled'] = None
                self.progress_signal.emit(100)
                
                self.result_signal.emit(results)
                
            else:
                # ê³ í™”ì§ˆ ì˜ìƒì€ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ
                self.log_signal.emit(f"âœ… ê³ í™”ì§ˆ ì˜ìƒ ({width}x{height}) - ì¼ë°˜ ë¶„ì„ ëª¨ë“œ")
                results = {
                    'original': self.analyze_video(preprocess_mode='none'),
                    'denoised': None,
                    'upscaled': None
                }
                self.progress_signal.emit(100)
                self.result_signal.emit(results)
                
        except Exception as e:
            self.error_signal.emit(f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}\n{traceback.format_exc()}")

    def init_sr_model(self):
        """EDSR Super Resolution ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            if not os.path.exists(EDSR_MODEL_PATH):
                self.log_signal.emit(f"   âš ï¸ SR ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {EDSR_MODEL_PATH}")
                return False
            
            self.sr_model = dnn_superres.DnnSuperResImpl_create()
            self.sr_model.readModel(EDSR_MODEL_PATH)
            self.sr_model.setModel("edsr", 4)
            return True
        except Exception as e:
            self.log_signal.emit(f"   âŒ SR ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False

    def apply_preprocessing(self, frame, mode):
        """í”„ë ˆì„ ì „ì²˜ë¦¬ ì ìš©"""
        if mode == 'none':
            return frame
        elif mode == 'denoise':
            # ë…¸ì´ì¦ˆ ì œê±°: Non-local Means Denoising
            return cv2.fastNlMeansDenoisingColored(frame, None, 10, 10, 7, 21)
        elif mode == 'upscale':
            if self.sr_model:
                return self.sr_model.upsample(frame)
            else:
                return frame
        return frame

    def analyze_video(self, preprocess_mode='none'):
        """ë‹¨ì¼ ë²„ì „ ì˜ìƒ ë¶„ì„"""
        try:
            # ëª¨ë¸ ë¡œë“œ
            self.log_signal.emit("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
            models = self.load_models()
            
            # íŠ¹ì§• ì¶”ì¶œ
            self.log_signal.emit("ğŸ¬ ì˜ìƒ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
            features = self.extract_features(preprocess_mode)
            
            if features is None:
                return {'error': 'íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨'}
            
            # ì˜ˆì¸¡
            self.log_signal.emit("ğŸ¤– ë”¥í˜ì´í¬ ë¶„ì„ ì¤‘...")
            scores = self.predict(models, features)
            
            # ê²°ê³¼ ê³„ì‚°
            result = self.calculate_final_result(scores)
            
            return result
            
        except Exception as e:
            return {'error': str(e)}

    def load_models(self):
        """ëª¨ë¸ ë¡œë“œ"""
        models = {}
        
        # XGBoost
        models['xgb'] = xgb.Booster()
        models['xgb'].load_model(MODEL_PATHS['HQ']['xgb'])
        
        # Tabular AE (120 features, 64 latent_dim)
        models['tab_ae'] = TabularAE(120, 64).to(device)
        models['tab_ae'].load_state_dict(torch.load(MODEL_PATHS['HQ']['tab_ae'], map_location=device))
        models['tab_ae'].eval()
        
        # RNN AE (GRU, hidden_dim=128, num_layers=2)
        models['rnn_ae'] = RNNAE('GRU', 128, 2, 5).to(device)
        models['rnn_ae'].load_state_dict(torch.load(MODEL_PATHS['HQ']['rnn_ae'], map_location=device))
        models['rnn_ae'].eval()
        
        # Multimodal AE (ìŒì„± ì‚¬ìš© ì‹œ)
        if self.use_audio:
            cfg = SimpleNamespace(
                cnn_latent_dim=64,      # 512 -> 64
                rnn_units=64,           # 256 -> 64
                bottleneck_dim=64,      # 128 -> 64
                rnn_model='LSTM'        # GRU -> LSTM (ë‹¤ì‹œ ìˆ˜ì •)
            )
            models['multi_ae'] = MultiModalAutoencoder(cfg).to(device)
            models['multi_ae'].load_state_dict(torch.load(MODEL_PATHS['HQ']['multi_ae'], map_location=device))
            models['multi_ae'].eval()
        
        # Scalers
        models['tab_scaler'] = joblib.load(MODEL_PATHS['HQ']['tab_scaler'])
        models['npy_scaler'] = joblib.load(MODEL_PATHS['HQ']['npy_scaler'])
        
        return models

    def extract_features(self, preprocess_mode):
        """íŠ¹ì§• ì¶”ì¶œ (ì „ì²˜ë¦¬ ëª¨ë“œ ì ìš©)"""
        try:
            cap = cv2.VideoCapture(self.video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            start_frame = int(self.start_time * fps)
            end_frame = int(self.end_time * fps)
            
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            # Face detector
            detector = dlib.get_frontal_face_detector()
            predictor = dlib.shape_predictor(DLIB_PATH)
            
            frame_features = []
            prev_regions = {}
            
            frame_idx = start_frame
            while frame_idx < end_frame:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # ì „ì²˜ë¦¬ ì ìš©
                frame = self.apply_preprocessing(frame, preprocess_mode)
                
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = detector(gray)
                
                if len(faces) > 0:
                    face = faces[0]
                    shape = predictor(gray, face)
                    
                    frame_feat = {}
                    for region_name, indices in FACIAL_LANDMARKS.items():
                        prev_mean = prev_regions.get(region_name)
                        region_feat = calculate_region_features(gray, shape, region_name, indices, prev_mean)
                        
                        if region_feat:
                            frame_feat[region_name] = region_feat
                            prev_regions[region_name] = region_feat['light_intensity_mean']
                    
                    if frame_feat:
                        frame_features.append(frame_feat)
                
                frame_idx += 1
            
            cap.release()
            
            if not frame_features:
                return None
            
            # Tabular íŠ¹ì§•
            tab_features = self.aggregate_tabular_features(frame_features)
            
            # NPY íŠ¹ì§• (ì‹œê³„ì—´)
            npy_features = self.create_npy_features(frame_features)
            
            # ìŒì„± íŠ¹ì§• (ì‚¬ìš© ì‹œ)
            audio_features = None
            if self.use_audio:
                audio_features = self.extract_audio_features()
            
            return {
                'tabular': tab_features,
                'npy': npy_features,
                'audio': audio_features
            }
            
        except Exception as e:
            self.log_signal.emit(f"âŒ íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return None

    def aggregate_tabular_features(self, frame_features):
        """í”„ë ˆì„ë³„ íŠ¹ì§•ì„ í†µí•©í•˜ì—¬ tabular í˜•íƒœë¡œ ë³€í™˜ (120ê°œ íŠ¹ì§•)"""
        all_values = {region: {key: [] for key in ['laplacian_mean', 'laplacian_var', 
                                                     'light_intensity_mean', 'light_intensity_change', 
                                                     'region_area']} 
                      for region in FACIAL_LANDMARKS.keys()}
        
        for frame_feat in frame_features:
            for region, feat in frame_feat.items():
                for key in feat.keys():
                    all_values[region][key].append(feat[key])
        
        aggregated = []
        # ê° region, ê° featureì— ëŒ€í•´ mean, std, min, max ê³„ì‚° (5 features * 4 stats = 20 per region)
        for region in FACIAL_LANDMARKS.keys():
            for key in ['laplacian_mean', 'laplacian_var', 'light_intensity_mean', 
                        'light_intensity_change', 'region_area']:
                values = all_values[region][key]
                if values:
                    aggregated.append(np.mean(values))   # mean
                    aggregated.append(np.std(values))    # std
                    aggregated.append(np.min(values))    # min
                    aggregated.append(np.max(values))    # max
                else:
                    aggregated.extend([0.0, 0.0, 0.0, 0.0])
        
        # 6 regions * 5 features * 4 stats = 120 features
        return np.array(aggregated).reshape(1, -1)

    def create_npy_features(self, frame_features, target_length=90):
        """ì‹œê³„ì—´ íŠ¹ì§• ìƒì„±"""
        npy_data = []
        
        for frame_feat in frame_features:
            full_face = frame_feat.get('full_face')
            if full_face:
                npy_data.append([
                    full_face['laplacian_mean'],
                    full_face['laplacian_var'],
                    full_face['light_intensity_mean'],
                    full_face['light_intensity_change'],
                    full_face['region_area']
                ])
        
        npy_array = np.array(npy_data)
        
        # ê¸¸ì´ ì¡°ì •
        if len(npy_array) < target_length:
            pad = np.zeros((target_length - len(npy_array), 5))
            npy_array = np.vstack([npy_array, pad])
        else:
            npy_array = npy_array[:target_length]
        
        return npy_array.reshape(1, target_length, 5)

    def extract_audio_features(self):
        """ìŒì„± íŠ¹ì§• ì¶”ì¶œ"""
        try:
            # Whisperë¡œ ìŒì„± í…ìŠ¤íŠ¸ ë³€í™˜
            model = whisper.load_model(WHISPER_SIZE)
            result = model.transcribe(self.video_path)
            
            # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
            y, sr = librosa.load(self.video_path, sr=16000, 
                                duration=self.end_time - self.start_time, 
                                offset=self.start_time)
            
            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            
            # 128x128ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            mel_resized = cv2.resize(mel_spec_db, (128, 128))
            mel_normalized = (mel_resized - mel_resized.min()) / (mel_resized.max() - mel_resized.min())
            
            return mel_normalized.reshape(1, 1, 128, 128)
            
        except Exception as e:
            self.log_signal.emit(f"âš ï¸ ìŒì„± íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None

    def predict(self, models, features):
        """ëª¨ë¸ ì˜ˆì¸¡"""
        scores = {}
        
        # Tabular ì •ê·œí™”
        tab_scaled = models['tab_scaler'].transform(features['tabular'])
        tab_tensor = torch.FloatTensor(tab_scaled).to(device)
        
        # NPY ì •ê·œí™”
        npy_scaled = models['npy_scaler'].transform(features['npy'].reshape(-1, 5)).reshape(1, 90, 5)
        npy_tensor = torch.FloatTensor(npy_scaled).to(device)
        
        # XGBoost ì˜ˆì¸¡
        dmat = xgb.DMatrix(tab_scaled)
        scores['xgb'] = models['xgb'].predict(dmat)[0]
        
        # Tabular AE ì˜ˆì¸¡
        with torch.no_grad():
            tab_recon = models['tab_ae'](tab_tensor)
            scores['tab'] = torch.mean((tab_tensor - tab_recon) ** 2).item()
        
        # RNN AE ì˜ˆì¸¡
        with torch.no_grad():
            rnn_recon = models['rnn_ae'](npy_tensor)
            scores['rnn'] = torch.mean((npy_tensor - rnn_recon) ** 2).item()
        
        # Multimodal AE ì˜ˆì¸¡ (ìŒì„± ì‚¬ìš© ì‹œ)
        if self.use_audio and features['audio'] is not None:
            audio_tensor = torch.FloatTensor(features['audio']).to(device)
            with torch.no_grad():
                img_recon, npy_recon = models['multi_ae'](audio_tensor, npy_tensor)
                scores['multi'] = (torch.mean((audio_tensor - img_recon) ** 2) + 
                                 torch.mean((npy_tensor - npy_recon) ** 2)).item()
        else:
            scores['multi'] = 0.0
        
        return scores

    def calculate_final_result(self, scores):
        """ìµœì¢… ê²°ê³¼ ê³„ì‚°"""
        report_lines = []
        
        def analyze_threshold(score, th_dict, name):
            if score <= th_dict['loose']:
                prob = 0.0
                status = "âœ… ì •ìƒ"
            elif score <= th_dict['strict']:
                prob = 0.3
                status = "âš ï¸ ì£¼ì˜"
            elif score <= th_dict['max']:
                prob = 0.7
                status = "ğŸ”´ ì˜ì‹¬"
            else:
                prob = 1.0
                status = "ğŸš¨ ë†’ìŒ"
            
            report_lines.append(f"[{name}] Score: {score:.4f} â†’ {status}")
            return prob
        
        th = THRESHOLDS
        
        p_xgb = scores['xgb']
        report_lines.append(f"[XGBoost] Prob: {p_xgb:.4f}")

        p_rnn = analyze_threshold(scores['rnn'], th['rnn'], "RNN Model")
        
        if self.use_audio and scores['multi'] > 0:
            p_multi = analyze_threshold(scores['multi'], th['multi'], "Multi Model")
            final_score = (p_xgb * WEIGHTS['xgb']) + (p_rnn * WEIGHTS['rnn']) + (p_multi * WEIGHTS['multi'])
        else:
            p_multi = 0.0
            report_lines.append("[Multi Model] OFF (ë¶„ì„ ì œì™¸)")
            w_xgb, w_rnn = 0.4, 0.6
            final_score = (p_xgb * w_xgb) + (p_rnn * w_rnn)

        details_str = "\n".join(report_lines)

        return {
            'final_prob': final_score * 100,
            'details': details_str,
            'raw_scores': scores
        }

# ============================================================
# 5. ë©”ì¸ ìœˆë„ìš° (GUI) - 3ê°€ì§€ ê²°ê³¼ í‘œì‹œ
# ============================================================
class DeepfakeApp(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Deepfake Detector Pro (HQ Model)")
        self.setGeometry(100, 100, 600, 700)
        self.initUI()
        self.video_path = None
        self.duration = 0

    def initUI(self):
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        layout = QVBoxLayout()
        
        lbl_title = QLabel("ë”¥í˜ì´í¬ íƒì§€ ì‹œìŠ¤í…œ (HQ ëª¨ë¸)")
        lbl_title.setAlignment(Qt.AlignCenter)
        lbl_title.setFont(QFont("Arial", 18, QFont.Bold))
        layout.addWidget(lbl_title)

        file_layout = QHBoxLayout()
        self.path_input = QLineEdit()
        self.path_input.setPlaceholderText("ê²€ì‚¬í•  ë¹„ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”...")
        btn_browse = QPushButton("íŒŒì¼ ì—´ê¸°")
        btn_browse.clicked.connect(self.open_file)
        file_layout.addWidget(self.path_input)
        file_layout.addWidget(btn_browse)
        layout.addLayout(file_layout)
        
        info_group = QGroupBox("ì˜ìƒ ì •ë³´ ë° í’ˆì§ˆ ìë™ ê°ì§€")
        info_layout = QVBoxLayout()
        self.lbl_info = QLabel("íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì£¼ì„¸ìš”.")
        self.lbl_quality = QLabel("í™”ì§ˆ: -")
        self.lbl_quality.setFont(QFont("Arial", 12, QFont.Bold))
        self.lbl_quality.setStyleSheet("color: gray")
        info_layout.addWidget(self.lbl_info)
        info_layout.addWidget(self.lbl_quality)
        info_group.setLayout(info_layout)
        layout.addWidget(info_group)
        
        opt_group = QGroupBox("ë¶„ì„ ì˜µì…˜")
        opt_layout = QHBoxLayout()
        
        opt_layout.addWidget(QLabel("ì‹œì‘(ì´ˆ):"))
        self.txt_start = QLineEdit("0")
        opt_layout.addWidget(self.txt_start)
        
        opt_layout.addWidget(QLabel("ì¢…ë£Œ(ì´ˆ):"))
        self.txt_end = QLineEdit("0")
        opt_layout.addWidget(self.txt_end)
        
        self.chk_audio = QCheckBox("ìŒì„± í¬í•¨ ì •ë°€ ê²€ì‚¬ (Multimodal)")
        self.chk_audio.setChecked(True)
        opt_layout.addWidget(self.chk_audio)
        
        opt_group.setLayout(opt_layout)
        layout.addWidget(opt_group)
        
        self.btn_run = QPushButton("ë”¥í˜ì´í¬ ë¶„ì„ ì‹œì‘")
        self.btn_run.setFixedHeight(50)
        self.btn_run.setFont(QFont("Arial", 12, QFont.Bold))
        self.btn_run.setStyleSheet("background-color: #007BFF; color: white; border-radius: 5px;")
        self.btn_run.clicked.connect(self.run_analysis)
        layout.addWidget(self.btn_run)
        
        self.progress = QProgressBar()
        layout.addWidget(self.progress)
        
        self.log_area = QTextEdit()
        self.log_area.setReadOnly(True)
        self.log_area.setFixedHeight(150)
        layout.addWidget(self.log_area)
        
        # ê²°ê³¼ í‘œì‹œ ì˜ì—­
        self.lbl_result = QLabel("ê²°ê³¼ ëŒ€ê¸° ì¤‘")
        self.lbl_result.setAlignment(Qt.AlignCenter)
        self.lbl_result.setFont(QFont("Arial", 16, QFont.Bold))
        self.lbl_result.setStyleSheet("border: 2px solid gray; padding: 15px; background-color: #f0f0f0;")
        layout.addWidget(self.lbl_result)
        
        # ìƒì„¸ ê²°ê³¼ í‘œì‹œ
        self.result_detail = QTextEdit()
        self.result_detail.setReadOnly(True)
        self.result_detail.setFixedHeight(200)
        self.result_detail.setStyleSheet("background-color: #f9f9f9; padding: 10px; font-size: 12px;")
        layout.addWidget(self.result_detail)
        
        central_widget.setLayout(layout)

    def open_file(self):
        fname, _ = QFileDialog.getOpenFileName(self, 'ë¹„ë””ì˜¤ ì„ íƒ', '', 'Video Files (*.mp4 *.avi *.mkv *.mov *.webm)')
        if fname:
            self.video_path = fname
            self.path_input.setText(fname)
            self.check_video_info()

    def check_video_info(self):
        cap = cv2.VideoCapture(self.video_path)
        if not cap.isOpened():
            self.log("âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.duration = frame_count / fps if fps > 0 else 0
        
        cap.release()
        
        # í™”ì§ˆ í‘œì‹œ ë¡œì§
        if width >= 1280 or height >= 720:
            q_text = "HIGH QUALITY (ê³ í™”ì§ˆ)"
            color = "green"
        else:
            q_text = "LOW QUALITY (ì €í™”ì§ˆ)"
            color = "orange"
            
        self.lbl_info.setText(f"í•´ìƒë„: {width}x{height} | FPS: {fps:.1f} | ê¸¸ì´: {self.duration:.2f}ì´ˆ")
        self.lbl_quality.setText(f"ìë™ ë¶„ë¥˜: {q_text}")
        self.lbl_quality.setStyleSheet(f"color: {color}")
        
        self.txt_end.setText(f"{self.duration:.1f}")
        self.log(f"ì˜ìƒ ë¡œë“œ ì™„ë£Œ: {q_text}")

    def log(self, msg):
        self.log_area.append(msg)
        self.log_area.verticalScrollBar().setValue(self.log_area.verticalScrollBar().maximum())

    def run_analysis(self):
        if not self.video_path:
            QMessageBox.warning(self, "ê²½ê³ ", "ë¹„ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
            
        try:
            start = float(self.txt_start.text())
            end = float(self.txt_end.text())
            if start < 0 or end > self.duration or start >= end:
                raise ValueError
        except:
            QMessageBox.warning(self, "ê²½ê³ ", "ì‹œì‘/ì¢…ë£Œ ì‹œê°„ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        self.btn_run.setEnabled(False)
        self.progress.setValue(0)
        self.lbl_result.setText("ë¶„ì„ ì¤‘...")
        self.lbl_result.setStyleSheet("background-color: #f0f0f0; border: 2px solid gray;")
        self.log_area.clear()
        self.result_detail.clear()
        
        # AnalysisThread ì‹¤í–‰
        self.worker = AnalysisThread(self.video_path, start, end, self.chk_audio.isChecked())
        self.worker.log_signal.connect(self.log)
        self.worker.progress_signal.connect(self.progress.setValue)
        self.worker.result_signal.connect(self.show_result)
        self.worker.error_signal.connect(self.handle_error)
        self.worker.start()

    def show_result(self, results):
        """ì›ë³¸ ê²°ê³¼ í‘œì‹œ"""
        
        # ì›ë³¸ ê²°ê³¼ë§Œ í‘œì‹œ
        if results.get('original'):
            orig = results['original']
            if 'error' not in orig:
                prob = orig['final_prob']
                details = orig['details']
                
                if prob >= 50:
                    verdict = "âš ï¸ ë”¥í˜ì´í¬ ì˜ì‹¬"
                    style = "background-color: #FFDDDD; color: red; border: 2px solid red;"
                else:
                    verdict = "âœ… ì •ìƒ ì˜ìƒ ê°€ëŠ¥ì„± ë†’ìŒ"
                    style = "background-color: #DDFFDD; color: green; border: 2px solid green;"
                
                self.lbl_result.setText(f"{verdict}\ní™•ë¥ : {prob:.2f}%")
                self.lbl_result.setStyleSheet(style)
                
                # ìƒì„¸ ê²°ê³¼
                detail_text = f"[ìƒì„¸ ì ìˆ˜]\n\n{details}"
                self.result_detail.setText(detail_text)
                
                report_msg = f"ë”¥í˜ì´í¬ í™•ë¥ : {prob:.2f}%\n\n[ìƒì„¸ ë¦¬í¬íŠ¸]\n{details}"
                self.log(f"\n{report_msg}")
                QMessageBox.information(self, "ë¶„ì„ ì™„ë£Œ", report_msg)
            else:
                self.lbl_result.setText(f"âŒ ì˜¤ë¥˜ ë°œìƒ")
                self.lbl_result.setStyleSheet("background-color: #FFE0E0; border: 2px solid red;")
                self.result_detail.setText(f"ì˜¤ë¥˜: {orig['error']}")
        
        self.btn_run.setEnabled(True)
        self.log("\nâœ… ë¶„ì„ ì™„ë£Œ!")

    def handle_error(self, msg):
        self.log(f"âŒ {msg}")
        self.btn_run.setEnabled(True)
        QMessageBox.critical(self, "ì˜¤ë¥˜ ë°œìƒ", msg)

if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = DeepfakeApp()
    window.show()
    sys.exit(app.exec_())