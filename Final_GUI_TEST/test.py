import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import cv2
import dlib
import librosa
import os
import tempfile
import joblib
import matplotlib.pyplot as plt
from types import SimpleNamespace

# ==========================================
# 1. ëª¨ë¸ ì„¤ì • ë° í´ë˜ìŠ¤ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ 100% ë™ì¼í•˜ê²Œ ìˆ˜ì •)
# ==========================================
# train_best_model.pyì˜ ì„¤ì •ê°’ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.
config = SimpleNamespace(
    batch_size = 32,
    bottleneck_dim = 64,
    cnn_latent_dim = 128, # í•™ìŠµ ì½”ë“œì— ë§ì¶° 128ë¡œ ì„¤ì •
    cnn_model = "LeNet",
    rnn_model = "GRU",
    rnn_units = 64
)

IMG_HEIGHT, IMG_WIDTH = 128, 128
NPY_SEQ_LENGTH, NPY_FEATURES = 90, 5

class MultiModalAutoencoder(nn.Module):
    def __init__(self, cfg):
        super(MultiModalAutoencoder, self).__init__()
        
        # --- [ìˆ˜ì •] í•™ìŠµ ì½”ë“œì˜ LeNet êµ¬ì¡° ì ìš© ---
        self.cnn_encoder = nn.Sequential(
            nn.Conv2d(1, 16, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2), # 64x64
            nn.Conv2d(16, 32, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2), # 32x32
            nn.Flatten(), 
            nn.Linear(32 * 32 * 32, cfg.cnn_latent_dim), nn.ReLU()
        )

        # --- [ìˆ˜ì •] í•™ìŠµ ì½”ë“œì˜ GRU êµ¬ì¡° ì ìš© ---
        self.rnn_encoder = nn.GRU(input_size=NPY_FEATURES, hidden_size=cfg.rnn_units, batch_first=True)
            
        # Bottleneck
        self.bottleneck = nn.Sequential(
            nn.Linear(cfg.cnn_latent_dim + cfg.rnn_units, cfg.bottleneck_dim), 
            nn.ReLU()
        )
        
        # RNN Decoder (GRU)
        self.rnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, cfg.rnn_units)
        self.rnn_decoder = nn.GRU(input_size=cfg.rnn_units, hidden_size=cfg.rnn_units, batch_first=True)
        self.rnn_output_layer = nn.Linear(cfg.rnn_units, NPY_FEATURES)
        
        # CNN Decoder
        self.cnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, 64 * 16 * 16)
        self.cnn_decoder = nn.Sequential(
            nn.Unflatten(1, (64, 16, 16)),
            nn.ConvTranspose2d(64, 64, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, 2, 1, 1), nn.ReLU(),
            nn.Conv2d(16, 1, 3, 1, 1), nn.Sigmoid()
        )
        
    def forward(self, img, npy):
        # Encoding
        cnn_feat = self.cnn_encoder(img)
        _, h_n = self.rnn_encoder(npy) # GRUëŠ” h_në§Œ ë°˜í™˜
        
        # Fusion (h_n[-1] ì‚¬ìš©)
        z = self.bottleneck(torch.cat((cnn_feat, h_n[-1]), dim=1))
        
        # Decoding
        rnn_in = self.rnn_decoder_fc(z).unsqueeze(1).repeat(1, NPY_SEQ_LENGTH, 1)
        rnn_out, _ = self.rnn_decoder(rnn_in)
        
        cnn_out = self.cnn_decoder(self.cnn_decoder_fc(z))
        
        return cnn_out, self.rnn_output_layer(rnn_out)

# ==========================================
# 2. íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ (Matplotlib ë²„ê·¸ ìˆ˜ì • í¬í•¨)
# ==========================================
DLIB_PATH = "shape_predictor_68_face_landmarks.dat"
FACIAL_LANDMARKS = {"mouth": list(range(48, 68))}

def get_region_bounding_box(shape, landmark_indices):
    points = [(shape.part(i).x, shape.part(i).y) for i in landmark_indices]
    xs, ys = zip(*points)
    return (min(xs), min(ys), max(xs), max(ys))

def extract_features(video_path, start_sec, end_sec, predictor_path):
    if not os.path.exists(predictor_path):
        return None, None, "Dlib ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor(predictor_path)

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    start_frame = int(start_sec * fps)
    end_frame = int(end_sec * fps)
    
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    
    mouth_features = []
    prev_light_mean = None
    frames_to_read = end_frame - start_frame
    
    # ì˜¤ë””ì˜¤ ì²˜ë¦¬
    try:
        y, sr = librosa.load(video_path, sr=44100, offset=start_sec, duration=(end_sec - start_sec))
        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
        S_dB = librosa.power_to_db(S, ref=np.max)
        
        fig = plt.figure(figsize=(10, 4))
        librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
        plt.axis('off')
        fig.canvas.draw()
        
        # [ìˆ˜ì •] ìµœì‹  matplotlib ëŒ€ì‘
        img_np = np.array(fig.canvas.renderer.buffer_rgba())
        plt.close(fig)
        
        img_gray = cv2.cvtColor(img_np, cv2.COLOR_RGBA2GRAY)
        img_resized = cv2.resize(img_gray, (IMG_WIDTH, IMG_HEIGHT))
        img_normalized = img_resized / 255.0
        img_tensor = torch.tensor(img_normalized, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        
    except Exception as e:
        return None, None, f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"

    # ë¹„ë””ì˜¤ í”„ë ˆì„ ì²˜ë¦¬
    count = 0
    for i in range(frames_to_read):
        ret, frame = cap.read()
        if not ret: break
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = detector(gray)
        if len(faces) == 0: continue
        
        shape = predictor(gray, faces[0])
        indices = FACIAL_LANDMARKS['mouth']
        x1, y1, x2, y2 = get_region_bounding_box(shape, indices)
        
        h, w = gray.shape
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)
        
        region = gray[y1:y2, x1:x2]
        if region.size == 0: continue
        
        laplacian = cv2.Laplacian(region, cv2.CV_64F)
        l_mean = np.abs(laplacian).mean()
        l_var = laplacian.var()
        light_mean = region.mean()
        light_change = (light_mean - prev_light_mean) if prev_light_mean is not None else 0.0
        prev_light_mean = light_mean
        area = (x2 - x1) * (y2 - y1)
        
        mouth_features.append([l_mean, l_var, light_mean, light_change, area])
        count += 1

    cap.release()
    if count < 10:
        return None, None, "ì–¼êµ´ì´ ê°ì§€ëœ í”„ë ˆì„ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤."

    return img_tensor, np.array(mouth_features), None

# ==========================================
# 3. Streamlit UI (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ)
# ==========================================
st.set_page_config(page_title="ë”¥í˜ì´í¬ íƒì§€ê¸°", layout="wide")
st.title("ğŸ•µï¸â€â™‚ï¸ ë”¥í˜ì´í¬ íƒì§€ê¸° (LeNet+GRU Model)")
st.markdown("í•™ìŠµëœ **LeNet + GRU** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ ë¶€ì¡°í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ì„¤ì •")
    # ì‚¬ìš©ìê°€ ì§ì ‘ ê°’ì„ ë³´ê³  ì¡°ì ˆí•  ìˆ˜ ìˆë„ë¡ ì…ë ¥ì°½ ì œê³µ
    threshold = st.number_input("ì˜ì‹¬ ê¸°ì¤€ê°’ (Threshold)", value=0.0050, format="%.6f", step=0.0001)
    st.info(f"Loss ê°’ì´ **{threshold:.6f}** ë³´ë‹¤ í¬ë©´ ë”¥í˜ì´í¬ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.")

uploaded_file = st.file_uploader("ë¹„ë””ì˜¤ ì—…ë¡œë“œ", type=["mp4", "avi", "mov"])

if uploaded_file is not None:
    tfile = tempfile.NamedTemporaryFile(delete=False) 
    tfile.write(uploaded_file.read())
    video_path = tfile.name
    
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = frame_count / fps
    cap.release()

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("1. ì›ë³¸ ì˜ìƒ")
        st.video(uploaded_file)
    
    with col2:
        st.subheader("2. ë¶„ì„ êµ¬ê°„")
        range_val = st.slider("êµ¬ê°„ ì„ íƒ (ì´ˆ)", 0.0, duration, (0.0, min(duration, 5.0)))
        start_sec, end_sec = range_val
        
        if st.button("ğŸš€ ë¶„ì„ ì‹œì‘"):
            if end_sec - start_sec < 1.0:
                st.error("ìµœì†Œ 1ì´ˆ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner('ë¶„ì„ ì¤‘... (ëª¨ë¸ ë¡œë”© ë° íŠ¹ì§• ì¶”ì¶œ)'):
                    try:
                        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                        
                        # ëª¨ë¸ ë¡œë“œ
                        model = MultiModalAutoencoder(config).to(device)
                        # [ì£¼ì˜] íŒŒì¼ëª…ì´ train_best_model.pyì—ì„œ ì €ì¥í•œ ì´ë¦„ê³¼ ê°™ì€ì§€ í™•ì¸í•˜ì„¸ìš”
                        model.load_state_dict(torch.load("best_deepfake_model.pt", map_location=device))
                        model.eval()
                        
                        # Scaler ë¡œë“œ (íŒŒì¼ëª… í™•ì¸ í•„ìš”: npy_scaler_final.joblib ë˜ëŠ” npy_scaler.joblib)
                        scaler_path = "npy_scaler_final.joblib"
                        if not os.path.exists(scaler_path):
                             scaler_path = "npy_scaler.joblib" # ì´ë¦„ì´ ë‹¤ë¥¼ ê²½ìš° ëŒ€ë¹„
                        scaler = joblib.load(scaler_path)
                        
                        # íŠ¹ì§• ì¶”ì¶œ
                        img_tensor, npy_raw, err = extract_features(video_path, start_sec, end_sec, DLIB_PATH)
                        
                        if err:
                            st.error(err)
                        else:
                            # ì „ì²˜ë¦¬
                            npy_scaled = scaler.transform(npy_raw)
                            curr = npy_scaled.shape[0]
                            padded = np.zeros((NPY_SEQ_LENGTH, NPY_FEATURES))
                            if curr > NPY_SEQ_LENGTH: padded = npy_scaled[:NPY_SEQ_LENGTH, :]
                            else: padded[:curr, :] = npy_scaled
                            
                            npy_tensor = torch.tensor(padded, dtype=torch.float32).unsqueeze(0).to(device)
                            img_tensor = img_tensor.to(device)
                            
                            # ì¶”ë¡ 
                            with torch.no_grad():
                                r_img, r_npy = model(img_tensor, npy_tensor)
                                l_img = nn.MSELoss()(r_img, img_tensor).item()
                                l_npy = nn.MSELoss()(r_npy, npy_tensor).item()
                                total_loss = l_img + l_npy
                            
                            # ê²°ê³¼ í‘œì‹œ
                            st.divider()
                            st.metric(label="ì´ ë³µì› ì˜¤ì°¨ (Loss)", value=f"{total_loss:.6f}")
                            
                            c1, c2 = st.columns(2)
                            c1.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì˜¤ì°¨: {l_img:.6f}")
                            c2.info(f"ğŸ“ˆ ì‹œê³„ì—´ ì˜¤ì°¨: {l_npy:.6f}")
                            
                            # ì‹œê°ì  íŒë‹¨
                            diff = total_loss - threshold
                            if total_loss > threshold:
                                st.error(f"ğŸš¨ **Deepfake ì˜ì‹¬** (ê¸°ì¤€ë³´ë‹¤ +{diff:.6f} ë†’ìŒ)")
                            else:
                                st.success(f"âœ… **ì •ìƒ(Real) ì¶”ì •** (ê¸°ì¤€ë³´ë‹¤ {diff:.6f} ë‚®ìŒ)")

                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
                        st.warning("íŒ: ëª¨ë¸ íŒŒì¼ëª…(best_deepfake_model.pt)ì´ë‚˜ Scaler íŒŒì¼ëª…ì´ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")