# build_engine1.py
# (ì—”ì§„ 1: XGBoost + Tabular AE + GRU AE ìµœì¢… í•™ìŠµ ë° ì €ìž¥)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
import numpy as np
import os
import xgboost as xgb
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from joblib import dump # ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ì €ìž¥ìš©
from tqdm import tqdm

# --- 1. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° (ì‚¬ìš©ìž ì œê³µ) ---
CONFIG = {
    'dl_batch_size': 64,
    'dl_learning_rate': 0.00262, # (ë°˜ì˜¬ë¦¼)
    'rnn_hidden_dim': 128,
    'rnn_layers': 2,
    'rnn_type': "GRU",
    'tab_latent_dim': 64,
    'xgb_learning_rate': 0.2,
    'xgb_max_depth': 3,
    'xgb_n_estimators': 200
}

# --- 2. ê²½ë¡œ ì„¤ì • ---
CSV_FILE_PATH = "./master_summary_v11_cleaned_final.csv" # (43,000ê°œ ë°ì´í„°)
NPY_DIR = "./2_npy_timeseries"
MODEL_SAVE_DIR = "./models/engine1" # ì €ìž¥ ê²½ë¡œ

# NPY ì„¤ì •
NPY_SEQ_LENGTH = 90
NPY_FEATURES = 5

# GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ðŸš€ Engine 1 í•™ìŠµ ì‹œìž‘ (Device: {device})")

# --- 3. ë°ì´í„° ë¡œë“œ (RAM ìµœì í™” ë²„ì „) ---
def load_data():
    print("ðŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv(CSV_FILE_PATH)
    num_cols = df.select_dtypes(include=[np.number]).columns
    df[num_cols] = df[num_cols].fillna(0)
    feat_cols = [c for c in num_cols if c not in ['label']]
    
    # 1. Tabular Scaler
    print("ðŸ“Š Tabular Scaler í”¼íŒ…...")
    tab_scaler = StandardScaler().fit(df[feat_cols])
    X_tab = tab_scaler.transform(df[feat_cols])
    
    # 2. Pseudo-labeling (XGBoostìš©)
    print("ðŸŒ² Isolation Forest ë¼ë²¨ë§...")
    iso = IsolationForest(contamination=0.05, random_state=42, n_jobs=-1)
    y_pseudo = (iso.fit_predict(X_tab) == -1).astype(int)
    
    # 3. NPY ë¡œë“œ
    print("ðŸ“¥ NPY íŒŒì¼ ë¡œë“œ ì¤‘...")
    X_npy = np.zeros((len(df), NPY_SEQ_LENGTH, NPY_FEATURES), dtype=np.float32)
    samples_for_scaler = []
    
    for idx, row in tqdm(df.iterrows(), total=len(df)):
        try:
            path = os.path.join(NPY_DIR, f"{row['video_id']}.npy")
            if os.path.exists(path):
                d = np.load(path, allow_pickle=True).item()
                m = np.stack([
                    d['mouth']['laplacian_mean'], d['mouth']['laplacian_var'],
                    d['mouth']['light_intensity_mean'], d['mouth']['light_intensity_change'],
                    d['mouth']['area']
                ], axis=1)
                
                curr = m.shape[0]
                if curr > NPY_SEQ_LENGTH: m = m[:NPY_SEQ_LENGTH]
                elif curr < NPY_SEQ_LENGTH: m = np.vstack([m, np.zeros((NPY_SEQ_LENGTH-curr, NPY_FEATURES))])
                
                X_npy[idx] = m
                if len(samples_for_scaler) < 5000: samples_for_scaler.append(m)
        except: pass
        
    # 4. NPY Scaler
    print("ðŸ“‰ NPY Scaler í”¼íŒ…...")
    npy_scaler = StandardScaler()
    if samples_for_scaler:
        npy_scaler.fit(np.concatenate(samples_for_scaler))
        N, T, F = X_npy.shape
        X_npy = npy_scaler.transform(X_npy.reshape(-1, F)).reshape(N, T, F)
        
    return X_tab, X_npy, y_pseudo, tab_scaler, npy_scaler, len(feat_cols)

# --- 4. ëª¨ë¸ í´ëž˜ìŠ¤ ---
class TabularAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128), nn.ReLU(), nn.BatchNorm1d(128),
            nn.Linear(128, latent_dim), nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128), nn.ReLU(), nn.BatchNorm1d(128),
            nn.Linear(128, input_dim)
        )
    def forward(self, x): return self.decoder(self.encoder(x))

class RNNAE(nn.Module):
    def __init__(self, rnn_type, hidden_dim, num_layers):
        super().__init__()
        self.enc = nn.GRU(NPY_FEATURES, hidden_dim, num_layers, batch_first=True)
        self.dec = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.out = nn.Linear(hidden_dim, NPY_FEATURES)
    def forward(self, x):
        _, h = self.enc(x)
        h_rep = h[-1].unsqueeze(1).repeat(1, NPY_SEQ_LENGTH, 1)
        dec_out, _ = self.dec(h_rep)
        return self.out(dec_out)

# --- 5. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    if not os.path.exists(MODEL_SAVE_DIR):
        os.makedirs(MODEL_SAVE_DIR)
        
    # 1. ë°ì´í„° ì¤€ë¹„
    X_tab, X_npy, y, tab_scaler, npy_scaler, tab_dim = load_data()
    
    # 2. XGBoost í•™ìŠµ ë° ì €ìž¥
    print("\nðŸš€ [1/3] XGBoost í•™ìŠµ ì‹œìž‘...")
    xgb_model = xgb.XGBClassifier(
        n_estimators=CONFIG['xgb_n_estimators'],
        max_depth=CONFIG['xgb_max_depth'],
        learning_rate=CONFIG['xgb_learning_rate'],
        tree_method='hist', device="cuda", random_state=42
    )
    xgb_model.fit(X_tab, y)
    xgb_model.save_model(os.path.join(MODEL_SAVE_DIR, "xgb_model.json"))
    print("âœ… XGBoost ì €ìž¥ ì™„ë£Œ.")
    
    # 3. PyTorch ëª¨ë¸ í•™ìŠµ
    print("\nðŸš€ [2/3] PyTorch Deep Learning í•™ìŠµ ì‹œìž‘...")
    
    ds = TensorDataset(torch.FloatTensor(X_tab), torch.FloatTensor(X_npy))
    loader = DataLoader(ds, batch_size=CONFIG['dl_batch_size'], shuffle=True)
    
    model_tab = TabularAE(tab_dim, CONFIG['tab_latent_dim']).to(device)
    model_rnn = RNNAE(CONFIG['rnn_type'], CONFIG['rnn_hidden_dim'], CONFIG['rnn_layers']).to(device)
    
    optimizer = optim.Adam(list(model_tab.parameters()) + list(model_rnn.parameters()), lr=CONFIG['dl_learning_rate'])
    criterion = nn.MSELoss()
    
    epochs = 20 # (ìµœì¢… í•™ìŠµì´ë¯€ë¡œ ì¶©ë¶„ížˆ)
    model_tab.train(); model_rnn.train()
    
    for epoch in range(epochs):
        total_loss = 0
        for bx_tab, bx_npy in tqdm(loader, desc=f"Epoch {epoch+1}/{epochs}"):
            bx_tab, bx_npy = bx_tab.to(device), bx_npy.to(device)
            optimizer.zero_grad()
            loss = criterion(model_tab(bx_tab), bx_tab) + criterion(model_rnn(bx_npy), bx_npy)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"   Loss: {total_loss/len(loader):.4f}")
        
    # 4. PyTorch ëª¨ë¸ ë° Scaler ì €ìž¥
    torch.save(model_tab.state_dict(), os.path.join(MODEL_SAVE_DIR, "tabular_ae.pth"))
    torch.save(model_rnn.state_dict(), os.path.join(MODEL_SAVE_DIR, "rnn_ae.pth"))
    dump(tab_scaler, os.path.join(MODEL_SAVE_DIR, "tab_scaler.joblib"))
    dump(npy_scaler, os.path.join(MODEL_SAVE_DIR, "npy_scaler.joblib"))
    
    print("\nðŸŽ‰ Engine 1 (ì˜ìƒ/í†µê³„) êµ¬ì¶• ì™„ë£Œ! ëª¨ë“  íŒŒì¼ì´ ì €ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ðŸ“‚ ì €ìž¥ ìœ„ì¹˜: {MODEL_SAVE_DIR}")