import os
import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, models
import wandb
from tqdm import tqdm
import time

# ============================================================
# 0. ì „ì—­ ì„¤ì • (ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ì„¸ìš”!)
# ============================================================
DATA_DIR = "./3_audio_spectrograms"  # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ
IMAGE_SIZE = 128                      # ì´ë¯¸ì§€ í¬ê¸°
SILENCE_THRESHOLD = 10                # ì¹¨ë¬µ êµ¬ê°„ í•„í„°ë§ (0=ë¹„í™œì„±í™”)

# ============================================================
# 1. Dataset Class
# ============================================================
class MelSpectrogramDataset(Dataset):
    """ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì´ë¯¸ì§€ ë°ì´í„°ì…‹"""
    
    def __init__(self, data_dir, transform=None, silence_threshold=10):
        self.data_dir = data_dir
        self.transform = transform
        self.silence_threshold = silence_threshold
        
        self.image_paths = []
        for fname in os.listdir(data_dir):
            if fname.endswith('.png'):
                fpath = os.path.join(data_dir, fname)
                if not self._is_silence(fpath):
                    self.image_paths.append(fpath)
        
        print(f"âœ… ì´ {len(self.image_paths)}ê°œ ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ")
    
    def _is_silence(self, img_path):
        """ì¹¨ë¬µ êµ¬ê°„ ê°ì§€"""
        img = cv2.imread(img_path)
        if img is None:
            return True
        mean_intensity = np.mean(img)
        return mean_intensity < self.silence_threshold
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            image = self.transform(image)
        
        return image

# ============================================================
# 2. Multiple CNN Architectures
# ============================================================

class ResNetAutoencoder(nn.Module):
    """ResNet ê¸°ë°˜ ì˜¤í† ì¸ì½”ë”"""
    
    def __init__(self, latent_dim=128):
        super().__init__()
        
        # Encoder: Pretrained ResNet18
        resnet = models.resnet18(pretrained=True)
        self.encoder = nn.Sequential(
            *list(resnet.children())[:-2],  # Remove FC and AvgPool
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(512, latent_dim),
            nn.ReLU()
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 512 * 4 * 4),
            nn.ReLU(),
            nn.Unflatten(1, (512, 4, 4)),
            
            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # 8x8
            nn.ReLU(),
            nn.BatchNorm2d(256),
            
            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 16x16
            nn.ReLU(),
            nn.BatchNorm2d(128),
            
            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 32x32
            nn.ReLU(),
            nn.BatchNorm2d(64),
            
            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # 64x64
            nn.ReLU(),
            nn.BatchNorm2d(32),
            
            nn.ConvTranspose2d(32, 3, 4, 2, 1),  # 128x128
            nn.Sigmoid()
        )
    
    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed

class EfficientNetAutoencoder(nn.Module):
    """EfficientNet ê¸°ë°˜ ì˜¤í† ì¸ì½”ë”"""
    
    def __init__(self, latent_dim=128):
        super().__init__()
        
        # Encoder: Pretrained EfficientNet-B0
        efficientnet = models.efficientnet_b0(pretrained=True)
        self.encoder = nn.Sequential(
            efficientnet.features,
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(1280, latent_dim),
            nn.ReLU()
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 1280 * 4 * 4),
            nn.ReLU(),
            nn.Unflatten(1, (1280, 4, 4)),
            
            nn.ConvTranspose2d(1280, 512, 4, 2, 1),  # 8x8
            nn.ReLU(),
            nn.BatchNorm2d(512),
            
            nn.ConvTranspose2d(512, 256, 4, 2, 1),  # 16x16
            nn.ReLU(),
            nn.BatchNorm2d(256),
            
            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 32x32
            nn.ReLU(),
            nn.BatchNorm2d(128),
            
            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 64x64
            nn.ReLU(),
            nn.BatchNorm2d(64),
            
            nn.ConvTranspose2d(64, 3, 4, 2, 1),  # 128x128
            nn.Sigmoid()
        )
    
    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed

class VGGAutoencoder(nn.Module):
    """VGG16 ê¸°ë°˜ ì˜¤í† ì¸ì½”ë”"""
    
    def __init__(self, latent_dim=128):
        super().__init__()
        
        # Encoder: Pretrained VGG16
        vgg = models.vgg16(pretrained=True)
        self.encoder = nn.Sequential(
            vgg.features,
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(512, latent_dim),
            nn.ReLU()
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 512 * 4 * 4),
            nn.ReLU(),
            nn.Unflatten(1, (512, 4, 4)),
            
            nn.ConvTranspose2d(512, 256, 4, 2, 1),
            nn.ReLU(),
            nn.BatchNorm2d(256),
            
            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.ReLU(),
            nn.BatchNorm2d(128),
            
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            
            nn.ConvTranspose2d(32, 3, 4, 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed

class SimpleConvAutoencoder(nn.Module):
    """ê°„ë‹¨í•œ Conv ì˜¤í† ì¸ì½”ë” (ë² ì´ìŠ¤ë¼ì¸)"""
    
    def __init__(self, latent_dim=128):
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 64x64
            nn.ReLU(),
            nn.BatchNorm2d(32),
            
            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 32x32
            nn.ReLU(),
            nn.BatchNorm2d(64),
            
            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 16x16
            nn.ReLU(),
            nn.BatchNorm2d(128),
            
            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 8x8
            nn.ReLU(),
            nn.BatchNorm2d(256),
            
            nn.Flatten(),
            nn.Linear(256 * 8 * 8, latent_dim),
            nn.ReLU()
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256 * 8 * 8),
            nn.ReLU(),
            nn.Unflatten(1, (256, 8, 8)),
            
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(128),
            
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            
            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed

# ============================================================
# 3. Model Factory
# ============================================================
def create_model(model_name, latent_dim):
    """ëª¨ë¸ íŒ©í† ë¦¬"""
    models_dict = {
        'resnet18': ResNetAutoencoder,
        'efficientnet_b0': EfficientNetAutoencoder,
        'vgg16': VGGAutoencoder,
        'simple_conv': SimpleConvAutoencoder
    }
    
    if model_name not in models_dict:
        raise ValueError(f"Unknown model: {model_name}")
    
    return models_dict[model_name](latent_dim=latent_dim)

# ============================================================
# 4. Training with WandB
# ============================================================
def train_with_wandb(config=None):
    """WandBë¥¼ ì‚¬ìš©í•œ í•™ìŠµ"""
    
    with wandb.init(config=config):
        config = wandb.config
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ë°ì´í„° ë¡œë“œ
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((config.image_size, config.image_size)),
            transforms.RandomHorizontalFlip(p=0.5),  # Augmentation
            transforms.ToTensor(),
        ])
        
        dataset = MelSpectrogramDataset(
            data_dir=config.data_dir,
            transform=transform,
            silence_threshold=config.silence_threshold
        )
        
        # Train/Val split
        train_size = int(0.9 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=config.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        # ëª¨ë¸ ìƒì„±
        model = create_model(config.model_name, config.latent_dim)
        model.to(device)
        
        # WandBì— ëª¨ë¸ êµ¬ì¡° ë¡œê¹…
        wandb.watch(model, log='all', log_freq=100)
        
        # Loss & Optimizer
        criterion = nn.MSELoss()
        
        if config.optimizer == 'adam':
            optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
        elif config.optimizer == 'adamw':
            optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
        else:
            optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=config.weight_decay)
        
        # Scheduler
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5
        )
        
        # í•™ìŠµ ë£¨í”„
        best_val_loss = float('inf')
        
        for epoch in range(config.num_epochs):
            # ===== Training =====
            model.train()
            train_loss = 0.0
            
            train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.num_epochs} [Train]")
            for batch in train_pbar:
                images = batch.to(device)
                
                # Forward
                reconstructed = model(images)
                loss = criterion(reconstructed, images)
                
                # Backward
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                train_pbar.set_postfix({'loss': loss.item()})
            
            avg_train_loss = train_loss / len(train_loader)
            
            # ===== Validation =====
            model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{config.num_epochs} [Val]")
                for batch in val_pbar:
                    images = batch.to(device)
                    reconstructed = model(images)
                    loss = criterion(reconstructed, images)
                    val_loss += loss.item()
                    val_pbar.set_postfix({'loss': loss.item()})
            
            avg_val_loss = val_loss / len(val_loader)
            
            # Scheduler step
            scheduler.step(avg_val_loss)
            
            # WandB ë¡œê¹…
            wandb.log({
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'learning_rate': optimizer.param_groups[0]['lr']
            })
            
            # Best model ì €ì¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'best_model.pth'))
                wandb.log({'best_val_loss': best_val_loss})
            
            print(f"Epoch {epoch+1}: Train Loss={avg_train_loss:.6f}, Val Loss={avg_val_loss:.6f}")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'final_model.pth'))
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ë¡œê¹…
        wandb.log({'final_val_loss': avg_val_loss})
        
        return best_val_loss

# ============================================================
# 5. Phase 1: Sweep Configuration (ë¹ ë¥¸ íƒìƒ‰)
# ============================================================
sweep_config_phase1 = {
    'method': 'bayes',  # ë² ì´ì§€ì•ˆ ìµœì í™”
    'metric': {
        'name': 'best_val_loss',
        'goal': 'minimize'
    },
    'parameters': {
        'model_name': {
            'values': ['resnet18', 'efficientnet_b0', 'vgg16', 'simple_conv']
        },
        'latent_dim': {
            'values': [64, 128, 256, 512]
        },
        'learning_rate': {
            'distribution': 'log_uniform_values',
            'min': 0.0001,
            'max': 0.01
        },
        'batch_size': {
            'values': [16, 32, 64]
        },
        'optimizer': {
            'values': ['adam', 'adamw']
        },
        'weight_decay': {
            'distribution': 'log_uniform_values',
            'min': 0.00001,
            'max': 0.001
        },
        'num_epochs': {
            'value': 30  # Phase 1ì€ ë¹ ë¥´ê²Œ
        },
        'image_size': {
            'value': 128
        },
        'silence_threshold': {
            'value': 10
        },
        'data_dir': {
            'value': DATA_DIR  # ì „ì—­ ì„¤ì • ì‚¬ìš©
        }
    }
}

# ============================================================
# 6. Phase 2: Refined Sweep Configuration (ì •ë°€ íƒìƒ‰)
# ============================================================
def create_phase2_sweep_config(best_model, best_latent_dim, best_lr_range, best_wd_range, data_dir='./mel_spectrograms'):
    """Phase 1 ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Phase 2 sweep config ìƒì„±"""
    
    sweep_config_phase2 = {
        'method': 'bayes',
        'metric': {
            'name': 'best_val_loss',
            'goal': 'minimize'
        },
        'parameters': {
            'model_name': {
                'value': best_model  # ìµœì  ëª¨ë¸ ê³ ì •
            },
            'latent_dim': {
                'values': [max(32, best_latent_dim - 32), best_latent_dim, best_latent_dim + 32]  # ì¢ì€ ë²”ìœ„
            },
            'learning_rate': {
                'distribution': 'log_uniform_values',
                'min': best_lr_range[0],
                'max': best_lr_range[1]
            },
            'batch_size': {
                'values': [16, 32, 64]
            },
            'optimizer': {
                'values': ['adam', 'adamw']
            },
            'weight_decay': {
                'distribution': 'log_uniform_values',
                'min': best_wd_range[0],
                'max': best_wd_range[1]
            },
            'num_epochs': {
                'value': 50  # Phase 2ëŠ” ë” ê¸¸ê²Œ
            },
            'image_size': {
                'value': 128
            },
            'silence_threshold': {
                'value': 10
            },
            'data_dir': {
                'value': data_dir  # ë°ì´í„° ê²½ë¡œ ì¶”ê°€
            }
        }
    }
    
    return sweep_config_phase2

# ============================================================
# 7. Main Execution
# ============================================================
if __name__ == "__main__":
    
    print("="*60)
    print("ğŸ”§ ì„¤ì •")
    print("="*60)
    print(f"ë°ì´í„° ê²½ë¡œ: {DATA_DIR}")
    print(f"ì´ë¯¸ì§€ í¬ê¸°: {IMAGE_SIZE}x{IMAGE_SIZE}")
    print(f"ì¹¨ë¬µ ì„ê³„ê°’: {SILENCE_THRESHOLD}")
    print("="*60)
    
    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    if not os.path.exists(DATA_DIR):
        print(f"\nâŒ ì˜¤ë¥˜: ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")
        print("ğŸ’¡ ì½”ë“œ ìƒë‹¨ì˜ DATA_DIR ë³€ìˆ˜ë¥¼ ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”!")
        exit(1)
    
    # WandB ë¡œê·¸ì¸ (ì²« ì‹¤í–‰ ì‹œ API í‚¤ ì…ë ¥ í•„ìš”)
    # wandb.login()
    
    print("\n" + "="*60)
    print("ğŸš€ Phase 1: ë¹ ë¥¸ íƒìƒ‰ (ë‹¤ì–‘í•œ ëª¨ë¸ + ë„“ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°)")
    print("="*60)
    
    # Phase 1 Sweep ìƒì„±
    sweep_id_phase1 = wandb.sweep(
        sweep_config_phase1,
        project="audio-deepfake-detection-phase1"
    )
    
    # Phase 1 ì‹¤í–‰ (ê° ëª¨ë¸ë‹¹ 10íšŒ)
    wandb.agent(sweep_id_phase1, function=train_with_wandb, count=40)  # 4ê°œ ëª¨ë¸ * 10íšŒ
    
    print("\n" + "="*60)
    print("âœ… Phase 1 ì™„ë£Œ!")
    print("ğŸ“Š WandBì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  ìµœì  ì„¤ì •ì„ ì°¾ìœ¼ì„¸ìš”")
    print("="*60)
    
    # Phase 1 ê²°ê³¼ ë¶„ì„ (ìˆ˜ë™ìœ¼ë¡œ í™•ì¸ í›„ ì•„ë˜ ê°’ ì„¤ì •)
    # WandB ì›¹ì—ì„œ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ í™•ì¸í•œ í›„:
    """
    best_model = 'resnet18'  # ì˜ˆì‹œ
    best_latent_dim = 128
    best_lr_range = [0.0001, 0.001]
    best_wd_range = [0.00001, 0.0001]
    
    print("\n" + "="*60)
    print("ğŸ¯ Phase 2: ì •ë°€ íƒìƒ‰ (ìµœì  ëª¨ë¸ + ì¢ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°)")
    print("="*60)
    
    # Phase 2 Sweep ìƒì„±
    sweep_config_phase2 = create_phase2_sweep_config(
        best_model, best_latent_dim, best_lr_range, best_wd_range
    )
    
    sweep_id_phase2 = wandb.sweep(
        sweep_config_phase2,
        project="audio-deepfake-detection-phase2"
    )
    
    # Phase 2 ì‹¤í–‰ (100íšŒ)
    wandb.agent(sweep_id_phase2, function=train_with_wandb, count=100)
    
    print("\n" + "="*60)
    print("ğŸ‰ Phase 2 ì™„ë£Œ!")
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ë¥¼ WandBì—ì„œ í™•ì¸í•˜ì„¸ìš”")
    print("="*60)
    """