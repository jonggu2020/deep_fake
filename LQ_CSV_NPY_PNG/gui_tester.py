import os
# ============================================================
# âš ï¸ [ìˆ˜ì •ë¨] OpenMP ì¶©ëŒ í•´ê²° (ë°˜ë“œì‹œ ìµœìƒë‹¨ ìœ„ì¹˜)
# ============================================================
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import io
import cv2
import numpy as np
import torch
import torch.nn as nn
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
from PIL import Image, ImageTk
import matplotlib.pyplot as plt
import librosa
import librosa.display
from torchvision import transforms, models

# ============================================================
# 1. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•´ì•¼ í•¨)
# ============================================================
class EfficientNetAutoencoder(nn.Module):
    def __init__(self, latent_dim=256):
        super().__init__()
        
        # --- Encoder (Pre-trained EfficientNet-B0) ---
        # weights=Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê²½ê³  ë©”ì‹œì§€ ì œê±° (ì–´ì°¨í”¼ ë¡œë“œí•˜ë¯€ë¡œ)
        efficientnet = models.efficientnet_b0(weights=None) 
        self.encoder_features = efficientnet.features
        
        self.encoder_head = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(1280, latent_dim),
            nn.ReLU()
        )
        
        # --- Decoder ---
        self.decoder_input = nn.Linear(latent_dim, 1280 * 4 * 4)
        
        self.decoder_layers = nn.Sequential(
            nn.ReLU(),
            nn.Unflatten(1, (1280, 4, 4)),
            nn.ConvTranspose2d(1280, 512, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(512), nn.ReLU(),
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256), nn.ReLU(),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128), nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64), nn.ReLU(),
            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        x = self.encoder_features(x)
        latent = self.encoder_head(x)
        x = self.decoder_input(latent)
        reconstructed = self.decoder_layers(x)
        return reconstructed

# ============================================================
# 2. GUI ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤
# ============================================================
class AudioDeepfakeTesterApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Deepfake Audio Anomaly Detection Tester")
        self.root.geometry("900x750")
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸ Device: {self.device}")

        # ëª¨ë¸ ë¡œë“œ
        self.model = self.load_model("best_model_final.pth")
        
        # ë³€ìˆ˜ ì´ˆê¸°í™”
        self.video_path = None
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((128, 128)), # í•™ìŠµ ì„¤ì •ê³¼ ë™ì¼
            transforms.ToTensor(),
        ])

        # UI êµ¬ì„±
        self.create_widgets()

    def load_model(self, model_path):
        if not os.path.exists(model_path):
            messagebox.showerror("Error", f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{model_path}")
            return None
        
        try:
            model = EfficientNetAutoencoder(latent_dim=256).to(self.device)
            state_dict = torch.load(model_path, map_location=self.device)
            model.load_state_dict(state_dict)
            model.eval()
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return model
        except Exception as e:
            messagebox.showerror("Error", f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n{e}")
            return None

    def create_widgets(self):
        # 1. íŒŒì¼ ì„ íƒ ì˜ì—­
        frame_top = tk.Frame(self.root, pady=10)
        frame_top.pack(fill="x", padx=20)
        
        self.btn_load = tk.Button(frame_top, text="ğŸ“‚ ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°", command=self.select_video, bg="#e1e1e1", font=("Arial", 12))
        self.btn_load.pack(side="left")
        
        self.lbl_filepath = tk.Label(frame_top, text="ì„ íƒëœ íŒŒì¼ ì—†ìŒ", fg="gray", font=("Arial", 10))
        self.lbl_filepath.pack(side="left", padx=10)

        # 2. ì„¤ì • ì˜ì—­ (ì‹œì‘/ì¢…ë£Œ ì‹œê°„)
        frame_controls = tk.Frame(self.root, pady=10)
        frame_controls.pack(fill="x", padx=20)
        
        tk.Label(frame_controls, text="ì‹œì‘ ì‹œê°„(ì´ˆ):").pack(side="left")
        self.entry_start = tk.Entry(frame_controls, width=8)
        self.entry_start.insert(0, "0.0")
        self.entry_start.pack(side="left", padx=5)
        
        tk.Label(frame_controls, text="ì¢…ë£Œ ì‹œê°„(ì´ˆ):").pack(side="left", padx=(10, 0))
        self.entry_end = tk.Entry(frame_controls, width=8)
        self.entry_end.insert(0, "3.0")
        self.entry_end.pack(side="left", padx=5)

        self.btn_run = tk.Button(frame_controls, text="ğŸš€ ë¶„ì„ ì‹œì‘", command=self.run_inference, bg="#4CAF50", fg="white", font=("Arial", 12, "bold"))
        self.btn_run.pack(side="left", padx=20)

        # 3. ê²°ê³¼ í…ìŠ¤íŠ¸ ì˜ì—­
        frame_result = tk.Frame(self.root, pady=10, bg="#f0f0f0")
        frame_result.pack(fill="x", padx=20)
        
        self.lbl_loss = tk.Label(frame_result, text="Reconstruction Loss: -", font=("Arial", 14, "bold"), bg="#f0f0f0")
        self.lbl_loss.pack(pady=5)
        
        self.lbl_desc = tk.Label(frame_result, text="(Lossê°€ ë‚®ì„ìˆ˜ë¡ í•™ìŠµëœ ë°ì´í„°(Real)ì™€ ìœ ì‚¬í•¨)", font=("Arial", 10), bg="#f0f0f0", fg="gray")
        self.lbl_desc.pack(pady=2)

        # 4. ì´ë¯¸ì§€ ì‹œê°í™” ì˜ì—­ (Canvas)
        frame_images = tk.Frame(self.root)
        frame_images.pack(fill="both", expand=True, padx=20, pady=10)
        
        # ì›ë³¸ ì´ë¯¸ì§€
        self.panel_orig = tk.Label(frame_images, text="Original Spectrogram")
        self.panel_orig.pack(side="left", expand=True, fill="both")
        
        # ë³µì› ì´ë¯¸ì§€
        self.panel_recon = tk.Label(frame_images, text="Reconstructed Spectrogram")
        self.panel_recon.pack(side="right", expand=True, fill="both")

    def select_video(self):
        path = filedialog.askopenfilename(
            title="ë¹„ë””ì˜¤ íŒŒì¼ ì„ íƒ",
            filetypes=[("Video Files", "*.mp4 *.avi *.mov *.mkv *.webm")]
        )
        if path:
            self.video_path = path
            self.lbl_filepath.config(text=os.path.basename(path), fg="black")

    def get_spectrogram_image(self, audio_path, start_sec, end_sec):
        """
        í•™ìŠµ ë°ì´í„° ìƒì„± ë¡œì§ê³¼ 100% ë™ì¼í•˜ê²Œ ë©”ëª¨ë¦¬ ìƒì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            # 1. ì˜¤ë””ì˜¤ ë¡œë“œ (librosa)
            duration = end_sec - start_sec
            # warningsë¥¼ ì¼ì‹œì ìœ¼ë¡œ ë¬´ì‹œí•˜ê±°ë‚˜ soundfile ì‚¬ìš© ìœ ë„
            y, sr = librosa.load(audio_path, sr=44100, offset=start_sec, duration=duration)
            
            if len(y) == 0:
                return None, "ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

            # 2. ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë³€í™˜
            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
            S_dB = librosa.power_to_db(S, ref=np.max)

            # 3. Matplotlibë¡œ ì´ë¯¸ì§€ ê·¸ë¦¬ê¸° (ë©”ëª¨ë¦¬ ë²„í¼ ì‚¬ìš©)
            plt.figure(figsize=(10, 4))
            librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
            plt.axis('off')
            
            buf = io.BytesIO()
            plt.savefig(buf, format='png', dpi=100, bbox_inches='tight', pad_inches=0)
            plt.close()
            buf.seek(0)
            
            # 4. ë²„í¼ë¥¼ OpenCV í¬ë§·ìœ¼ë¡œ ë³€í™˜
            file_bytes = np.asarray(bytearray(buf.read()), dtype=np.uint8)
            img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR -> RGB (í•™ìŠµ ì½”ë“œì™€ ì¼ì¹˜)
            
            return img, None
            
        except Exception as e:
            return None, str(e)

    def run_inference(self):
        if not self.model:
            messagebox.showerror("Error", "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        if not self.video_path:
            messagebox.showwarning("Warning", "ë¹„ë””ì˜¤ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return

        try:
            start = float(self.entry_start.get())
            end = float(self.entry_end.get())
            if start >= end:
                messagebox.showwarning("Warning", "ì‹œì‘ ì‹œê°„ì€ ì¢…ë£Œ ì‹œê°„ë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.")
                return
        except ValueError:
            messagebox.showwarning("Warning", "ì‹œê°„ì€ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
            return

        # 1. ì „ì²˜ë¦¬ (ì´ë¯¸ì§€ ìƒì„±)
        self.btn_run.config(state="disabled", text="ë¶„ì„ ì¤‘...")
        self.root.update()

        img_rgb, error = self.get_spectrogram_image(self.video_path, start, end)
        
        if img_rgb is None:
            messagebox.showerror("Error", f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {error}")
            self.btn_run.config(state="normal", text="ğŸš€ ë¶„ì„ ì‹œì‘")
            return

        # 2. í…ì„œ ë³€í™˜
        input_tensor = self.transform(img_rgb).unsqueeze(0).to(self.device) # (1, 3, 128, 128)

        # 3. ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            reconstructed = self.model(input_tensor)
            loss = nn.MSELoss()(reconstructed, input_tensor).item()

        # 4. ê²°ê³¼ ì‹œê°í™”
        self.display_results(input_tensor, reconstructed, loss)
        self.btn_run.config(state="normal", text="ğŸš€ ë¶„ì„ ì‹œì‘")

    def display_results(self, original_tensor, recon_tensor, loss):
        # Loss í‘œì‹œ
        loss_str = f"{loss:.6f}"
        self.lbl_loss.config(text=f"Reconstruction Loss: {loss_str}")
        
        # Loss ìƒ‰ìƒ ì½”ë”© (ë‹¨ìˆœ ì˜ˆì‹œ ê¸°ì¤€, ì‹¤ì œ ë°ì´í„°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        # Autoencoderì—ì„œ í•™ìŠµí•˜ì§€ ì•Šì€ ë°ì´í„°(Fake)ëŠ” Lossê°€ ë†’ìŒ
        if loss > 0.01: # ì„ì˜ì˜ ì„ê³„ê°’ (ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸í•˜ë©° ê° ì¡ì•„ì•¼ í•¨)
            self.lbl_loss.config(fg="red")
            self.lbl_desc.config(text="ë†’ì€ ì˜¤ì°¨: í•™ìŠµ ë°ì´í„°ì™€ ë‹¤ë¦„ (ì ì¬ì  Fake/Anomaly)")
        else:
            self.lbl_loss.config(fg="green")
            self.lbl_desc.config(text="ë‚®ì€ ì˜¤ì°¨: í•™ìŠµ ë°ì´í„°ì™€ ìœ ì‚¬í•¨ (Real)")

        # Tensor -> PIL Image ë³€í™˜
        to_pil = transforms.ToPILImage()
        
        orig_img = to_pil(original_tensor.squeeze().cpu())
        recon_img = to_pil(recon_tensor.squeeze().cpu())

        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (í™”ë©´ì— ë§ê²Œ)
        disp_size = (400, 400)
        orig_img = orig_img.resize(disp_size)
        recon_img = recon_img.resize(disp_size)

        # Tkinter ì´ë¯¸ì§€ ê°ì²´ ìƒì„±
        self.tk_orig = ImageTk.PhotoImage(orig_img)
        self.tk_recon = ImageTk.PhotoImage(recon_img)

        # ë¼ë²¨ì— ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        self.panel_orig.config(image=self.tk_orig, text="[ì…ë ¥] Original Spectrogram", compound="top", font=("Arial", 12, "bold"))
        self.panel_orig.image = self.tk_orig # ì°¸ì¡° ìœ ì§€
        
        self.panel_recon.config(image=self.tk_recon, text="[ë³µì›] Autoencoder Reconstruction", compound="top", font=("Arial", 12, "bold"))
        self.panel_recon.image = self.tk_recon # ì°¸ì¡° ìœ ì§€


# ============================================================
# 3. ë©”ì¸ ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    root = tk.Tk()
    app = AudioDeepfakeTesterApp(root)
    root.mainloop()