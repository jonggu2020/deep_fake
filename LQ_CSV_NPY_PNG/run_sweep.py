import wandb
import pandas as pd
import numpy as np
import os
import cv2
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# --- 1. GPU ì„¤ì • ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# --- 2. ê²½ë¡œ ë° ì „ì—­ ì„¤ì • ---
CSV_FILE_PATH = "./final_cleaned_interactive.csv" 
NPY_DIR = "./2_npy_timeseries"
PNG_DIR = "./3_audio_spectrograms"
IMG_HEIGHT, IMG_WIDTH = 128, 128
NPY_SEQ_LENGTH, NPY_FEATURES = 90, 5 

# --- 3. Sweep ì„¤ì • (ì—¬ê¸°ì— íŠœë‹í•  ë²”ìœ„ë¥¼ ì •ì˜) ---
sweep_config = {
    'method': 'bayes',
    'metric': {'name': 'val_loss', 'goal': 'minimize'},
    'parameters': {
        'learning_rate': {'distribution': 'log_uniform_values', 'min': 1e-4, 'max': 1e-3},
        'batch_size': {'values': [16, 32, 64]},
        'cnn_model': {'values': ['LeNet', 'AlexNet_Mini']}, # VGGëŠ” ë¬´ê±°ìš°ë©´ ì œì™¸ ê°€ëŠ¥
        'cnn_latent_dim': {'values': [64, 128]},
        'rnn_model': {'values': ['LSTM', 'GRU']},
        'rnn_units': {'values': [32, 64]},
        'bottleneck_dim': {'values': [32, 64]}
    }
}

# --- 4. RAM ìºì‹± ë°ì´í„°ì…‹ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ---
class MultiModalRamDataset(Dataset):
    def __init__(self, df, npy_dir, png_dir, img_dims, npy_dims, scaler, mode='Train'):
        self.df = df.reset_index(drop=True)
        self.npy_dir = npy_dir
        self.png_dir = png_dir
        self.img_height, self.img_width = img_dims
        self.seq_len, self.n_features = npy_dims
        self.scaler = scaler
        self.cached_data = []
        
        print(f"[{mode}] ë°ì´í„°ë¥¼ RAMì— ë¡œë“œ ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)")
        for i in tqdm(range(len(self.df)), desc=f"Loading {mode}"):
            video_id = self.df.loc[i, 'video_id']
            
            # PNG ì²˜ë¦¬
            try:
                png_path = os.path.join(self.png_dir, f"{video_id}.png")
                img = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)
                if img is None: raise FileNotFoundError
                img = cv2.resize(img, (self.img_width, self.img_height))
                img_normalized = img / 255.0
                img_tensor = torch.tensor(img_normalized, dtype=torch.float32).unsqueeze(0)
            except:
                img_tensor = torch.zeros((1, self.img_height, self.img_width), dtype=torch.float32)
            
            # NPY ì²˜ë¦¬
            try:
                npy_path = os.path.join(self.npy_dir, f"{video_id}.npy")
                data = np.load(npy_path, allow_pickle=True).item()
                mouth_data = np.stack([
                    data['mouth']['laplacian_mean'], data['mouth']['laplacian_var'],
                    data['mouth']['light_intensity_mean'], data['mouth']['light_intensity_change'],
                    data['mouth']['area']
                ], axis=1)
                mouth_data_scaled = self.scaler.transform(mouth_data.reshape(-1, self.n_features))
                
                curr_len = mouth_data_scaled.shape[0]
                padded_data = np.zeros((self.seq_len, self.n_features))
                if curr_len > self.seq_len: padded_data = mouth_data_scaled[:self.seq_len, :]
                else: padded_data[:curr_len, :] = mouth_data_scaled
                npy_tensor = torch.tensor(padded_data, dtype=torch.float32)
            except:
                npy_tensor = torch.zeros((self.seq_len, self.n_features), dtype=torch.float32)
            
            self.cached_data.append((img_tensor, npy_tensor))
            
    def __len__(self): return len(self.cached_data)
    def __getitem__(self, index):
        return self.cached_data[index], self.cached_data[index] # Autoencoderë¼ ì…ë ¥=ì •ë‹µ

# --- 5. ëª¨ë¸ (Sweep íŒŒë¼ë¯¸í„° ì ìš©ë˜ë„ë¡ ìˆ˜ì •ë¨) ---
class MultiModalAutoencoder(nn.Module):
    def __init__(self, cfg):
        super(MultiModalAutoencoder, self).__init__()
        self.cfg = cfg
        
        # 1) CNN Encoder ì„ íƒ (Sweep Configì— ë”°ë¼ ë³€ê²½)
        cnn_type = getattr(cfg, 'cnn_model', 'AlexNet_Mini')
        
        if cnn_type == 'LeNet':
            self.cnn_encoder = nn.Sequential(
                nn.Conv2d(1, 16, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2), # 64x64
                nn.Conv2d(16, 32, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2), # 32x32
                nn.Flatten(), nn.Linear(32 * 32 * 32, cfg.cnn_latent_dim), nn.ReLU()
            )
        else: # AlexNet_Mini (Default)
            self.cnn_encoder = nn.Sequential(
                nn.Conv2d(1, 32, 5, 2, 0), nn.ReLU(), nn.MaxPool2d(2),
                nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),
                nn.Flatten(), nn.Linear(64 * 15 * 15, cfg.cnn_latent_dim), nn.ReLU()
            )

        # 2) RNN Encoder ì„ íƒ
        rnn_type = getattr(cfg, 'rnn_model', 'LSTM')
        if rnn_type == 'GRU':
            self.rnn_encoder = nn.GRU(input_size=NPY_FEATURES, hidden_size=cfg.rnn_units, batch_first=True)
        else:
            self.rnn_encoder = nn.LSTM(input_size=NPY_FEATURES, hidden_size=cfg.rnn_units, batch_first=True)
            
        # 3) Bottleneck
        self.bottleneck = nn.Sequential(nn.Linear(cfg.cnn_latent_dim + cfg.rnn_units, cfg.bottleneck_dim), nn.ReLU())
        
        # 4) Decoders
        self.rnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, cfg.rnn_units)
        if rnn_type == 'GRU':
            self.rnn_decoder = nn.GRU(input_size=cfg.rnn_units, hidden_size=cfg.rnn_units, batch_first=True)
        else:
            self.rnn_decoder = nn.LSTM(input_size=cfg.rnn_units, hidden_size=cfg.rnn_units, batch_first=True)
        self.rnn_output_layer = nn.Linear(cfg.rnn_units, NPY_FEATURES)
        
        # CNN Decoder (êµ¬ì¡° ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ê³µí†µ ì‚¬ìš©, í•„ìš”ì‹œ ë¶„ê¸° ê°€ëŠ¥)
        self.cnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, 64 * 16 * 16)
        self.cnn_decoder = nn.Sequential(
            nn.Unflatten(1, (64, 16, 16)),
            nn.ConvTranspose2d(64, 64, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, 2, 1, 1), nn.ReLU(),
            nn.Conv2d(16, 1, 3, 1, 1), nn.Sigmoid()
        )
        
    def forward(self, img, npy):
        cnn_feat = self.cnn_encoder(img)
        
        # RNN ë¶„ê¸° ì²˜ë¦¬ (LSTMì€ h, c ë°˜í™˜ / GRUëŠ” h ë°˜í™˜)
        if isinstance(self.rnn_encoder, nn.LSTM):
            _, (h_n, _) = self.rnn_encoder(npy)
        else:
            _, h_n = self.rnn_encoder(npy)
            
        # h_nì˜ shape: (num_layers, batch, hidden). ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ ì‚¬ìš© -> h_n[-1]
        z = self.bottleneck(torch.cat((cnn_feat, h_n[-1]), dim=1))
        
        rnn_in = self.rnn_decoder_fc(z).unsqueeze(1).repeat(1, NPY_SEQ_LENGTH, 1)
        rnn_out, _ = self.rnn_decoder(rnn_in)
        
        return self.cnn_decoder(self.cnn_decoder_fc(z)), self.rnn_output_layer(rnn_out)

# --- 6. ìœ í‹¸ë¦¬í‹° (EarlyStopping, Scaler) ---
class EarlyStopping:
    def __init__(self, patience=5, delta=0): # Sweep ì†ë„ë¥¼ ìœ„í•´ patience 5ë¡œ ì¡°ì •
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.delta = delta
    def __call__(self, val_loss, model):
        score = -val_loss
        if self.best_score is None: self.best_score = score
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.counter >= self.patience: self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0

def get_npy_scaler(df, npy_dir):
    scaler_path = "npy_scaler.joblib"
    if os.path.exists(scaler_path):
        from joblib import load
        return load(scaler_path)
    from joblib import dump
    scaler = StandardScaler()
    sample_ids = df['video_id'].sample(min(len(df), 1500), random_state=42)
    all_npy = []
    for vid in sample_ids:
        try:
            d = np.load(os.path.join(npy_dir, f"{vid}.npy"), allow_pickle=True).item()['mouth']
            all_npy.append(np.stack([d['laplacian_mean'], d['laplacian_var'], d['light_intensity_mean'], d['light_intensity_change'], d['area']], axis=1))
        except: pass
    scaler.fit(np.concatenate(all_npy).reshape(-1, NPY_FEATURES))
    dump(scaler, scaler_path)
    return scaler

# --- 7. ë°ì´í„° ë¡œë“œ (ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•˜ì—¬ Sweep ë°˜ë³µ ì‹œ ì¬ì‚¬ìš©) ---
# ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸í•´ë‘ë©´ agentê°€ train_sweep í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•´ë„ 
# ì´ ë¶€ë¶„ì€ ë‹¤ì‹œ ì‹¤í–‰ë˜ì§€ ì•Šì•„ RAM ë¡œë”© ì‹œê°„ì„ ì•„ë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
print("ğŸš€ [ì‹œìŠ¤í…œ] ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì¤‘... (ì´ ê³¼ì •ì€ í•œ ë²ˆë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤)")
if os.path.exists(CSV_FILE_PATH):
    df_all = pd.read_csv(CSV_FILE_PATH)
    df_train, df_val = train_test_split(df_all, test_size=0.2, random_state=42)
    scaler = get_npy_scaler(df_train, NPY_DIR)
    
    # â˜… ë°ì´í„°ì…‹ ë¯¸ë¦¬ ë¡œë“œ (RAM ìƒì£¼)
    train_dataset_global = MultiModalRamDataset(df_train, NPY_DIR, PNG_DIR, (IMG_HEIGHT, IMG_WIDTH), (NPY_SEQ_LENGTH, NPY_FEATURES), scaler, mode='Train')
    val_dataset_global = MultiModalRamDataset(df_val, NPY_DIR, PNG_DIR, (IMG_HEIGHT, IMG_WIDTH), (NPY_SEQ_LENGTH, NPY_FEATURES), scaler, mode='Val')
    print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
else:
    print(f"âŒ [ì˜¤ë¥˜] CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CSV_FILE_PATH}")
    train_dataset_global, val_dataset_global = None, None

# --- 8. Sweepìš© í•™ìŠµ í•¨ìˆ˜ ---
def train_sweep():
    # WandB ì´ˆê¸°í™” (Sweep Agentê°€ ì„¤ì •ì„ ì£¼ì…í•¨)
    wandb.init()
    config = wandb.config
    
    seed_everything(42)
    
    # DataLoader ìƒì„± (Batch SizeëŠ” íŠœë‹ ëŒ€ìƒì´ë¯€ë¡œ ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±)
    if train_dataset_global is None: return
    
    train_loader = DataLoader(train_dataset_global, batch_size=config.batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset_global, batch_size=config.batch_size, shuffle=False, num_workers=0)
    
    # ëª¨ë¸ ìƒì„±
    model = MultiModalAutoencoder(config).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
    early_stopping = EarlyStopping(patience=5)
    
    # í•™ìŠµ ë£¨í”„ (EpochsëŠ” ë³´í†µ 10~15 ì •ë„ë¡œ ê³ ì •í•˜ê±°ë‚˜ configì— ì¶”ê°€ ê°€ëŠ¥)
    epochs = 10 
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        
        for (img_in, npy_in), (img_t, npy_t) in train_loader:
            img_in, npy_in, img_t, npy_t = img_in.to(device), npy_in.to(device), img_t.to(device), npy_t.to(device)
            optimizer.zero_grad()
            p_out, n_out = model(img_in, npy_in)
            loss = criterion(p_out, img_t) + criterion(n_out, npy_t)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for (img_in, npy_in), (img_t, npy_t) in val_loader:
                img_in, npy_in, img_t, npy_t = img_in.to(device), npy_in.to(device), img_t.to(device), npy_t.to(device)
                p_out, n_out = model(img_in, npy_in)
                val_loss += (criterion(p_out, img_t) + criterion(n_out, npy_t)).item()
                
        avg_train = train_loss / len(train_loader)
        avg_val = val_loss / len(val_loader)
        
        # WandB ê¸°ë¡
        wandb.log({"epoch": epoch+1, "train_loss": avg_train, "val_loss": avg_val})
        
        # Early Stopping ì²´í¬
        early_stopping(avg_val, model)
        if early_stopping.early_stop:
            print(f"â¹ Early Stopping at Epoch {epoch+1}")
            break

# --- 9. ë©”ì¸ ì‹¤í–‰ë¶€ ---
if __name__ == "__main__":
    if train_dataset_global is None:
        print("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()

    print("\nğŸš€ Sweep Agent ì‹œì‘! (WandB ëŒ€ì‹œë³´ë“œì—ì„œ ì§„í–‰ìƒí™© í™•ì¸ ê°€ëŠ¥)")
    
    # Sweep ë“±ë¡
    sweep_id = wandb.sweep(sweep_config, project="deepfake-LQ-CNN-model_2")
    
    # Agent ì‹¤í–‰ (count=10: ì´ 100ë²ˆì˜ ë‹¤ë¥¸ ì¡°í•© ì‹œë„)
    wandb.agent(sweep_id, function=train_sweep, count=100)