# calculate_threshold_lenet_gru.py
# (LeNet + GRU ëª¨ë¸ìš© ì„ê³„ê°’ ê³„ì‚° ì½”ë“œ)

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import os
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader
from types import SimpleNamespace
from joblib import load
from tqdm import tqdm

# --- 1. ì„¤ì • ë° ê²½ë¡œ (train_best_model.pyì™€ ë™ì¼í•˜ê²Œ ì„¤ì •) ---
config = SimpleNamespace(
    batch_size = 256,          # ì¶”ë¡  ì‹œì—ëŠ” ë°°ì¹˜ë¥¼ í‚¤ì›Œë„ ë¬´ë°©í•¨
    bottleneck_dim = 64,
    cnn_latent_dim = 128,      # í•™ìŠµ ì½”ë“œì— ë§ì¶° ìˆ˜ì • (64 -> 128)
    cnn_model = "LeNet",       # í•™ìŠµ ì½”ë“œì— ë§ì¶° ìˆ˜ì •
    rnn_model = "GRU",         # í•™ìŠµ ì½”ë“œì— ë§ì¶° ìˆ˜ì •
    rnn_units = 64
)

# íŒŒì¼ ê²½ë¡œ (í•™ìŠµ ì½”ë“œ ê¸°ì¤€)
CSV_FILE_PATH = "./final_cleaned_interactive.csv" 
NPY_DIR = "./2_npy_timeseries"
PNG_DIR = "./3_audio_spectrograms"
MODEL_PATH = "best_deepfake_model.pt"      # í•™ìŠµëœ ëª¨ë¸ íŒŒì¼
SCALER_PATH = "npy_scaler_final.joblib"    # í•™ìŠµëœ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼

IMG_HEIGHT, IMG_WIDTH = 128, 128
NPY_SEQ_LENGTH, NPY_FEATURES = 90, 5 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (train_best_model.pyì™€ 100% ì¼ì¹˜) ---
class MultiModalAutoencoder(nn.Module):
    def __init__(self, cfg):
        super(MultiModalAutoencoder, self).__init__()
        
        # 1) CNN Encoder: LeNet
        self.cnn_encoder = nn.Sequential(
            nn.Conv2d(1, 16, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2), # 64x64
            nn.Conv2d(16, 32, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2), # 32x32
            nn.Flatten(), 
            nn.Linear(32 * 32 * 32, cfg.cnn_latent_dim), nn.ReLU()
        )

        # 2) RNN Encoder: GRU
        self.rnn_encoder = nn.GRU(input_size=NPY_FEATURES, hidden_size=cfg.rnn_units, batch_first=True)
            
        # 3) Bottleneck (Fusion)
        self.bottleneck = nn.Sequential(
            nn.Linear(cfg.cnn_latent_dim + cfg.rnn_units, cfg.bottleneck_dim), 
            nn.ReLU()
        )
        
        # 4) RNN Decoder (GRU)
        self.rnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, cfg.rnn_units)
        self.rnn_decoder = nn.GRU(input_size=cfg.rnn_units, hidden_size=cfg.rnn_units, batch_first=True)
        self.rnn_output_layer = nn.Linear(cfg.rnn_units, NPY_FEATURES)
        
        # 5) CNN Decoder
        self.cnn_decoder_fc = nn.Linear(cfg.bottleneck_dim, 64 * 16 * 16)
        self.cnn_decoder = nn.Sequential(
            nn.Unflatten(1, (64, 16, 16)),
            nn.ConvTranspose2d(64, 64, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, 2, 1, 1), nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, 2, 1, 1), nn.ReLU(),
            nn.Conv2d(16, 1, 3, 1, 1), nn.Sigmoid()
        )
        
    def forward(self, img, npy):
        # Encoding
        cnn_feat = self.cnn_encoder(img)
        _, h_n = self.rnn_encoder(npy) # GRUëŠ” h_në§Œ ë°˜í™˜
        
        # Fusion
        # h_n shape: (num_layers, batch, hidden) -> ë§¨ ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚¬ìš©
        z = self.bottleneck(torch.cat((cnn_feat, h_n[-1]), dim=1))
        
        # Decoding (RNN)
        rnn_in = self.rnn_decoder_fc(z).unsqueeze(1).repeat(1, NPY_SEQ_LENGTH, 1)
        rnn_out, _ = self.rnn_decoder(rnn_in)
        
        # Decoding (CNN)
        cnn_out = self.cnn_decoder(self.cnn_decoder_fc(z))
        
        return cnn_out, self.rnn_output_layer(rnn_out)

# --- 3. ë°ì´í„°ì…‹ ì •ì˜ (ì¶”ë¡ ìš©) ---
class InferenceDataset(Dataset):
    def __init__(self, df, npy_dir, png_dir, scaler):
        self.df = df.reset_index(drop=True)
        self.npy_dir = npy_dir
        self.png_dir = png_dir
        self.scaler = scaler
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, index):
        video_id = self.df.loc[index, 'video_id']
        
        # PNG ë¡œë“œ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼ ì „ì²˜ë¦¬)
        try:
            path = os.path.join(self.png_dir, f"{video_id}.png")
            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if img is None: raise Exception
            img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))
            img = img / 255.0  # Normalize
            img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0)
        except:
            img_tensor = torch.zeros((1, IMG_HEIGHT, IMG_WIDTH), dtype=torch.float32)

        # NPY ë¡œë“œ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼ ì „ì²˜ë¦¬)
        try:
            path = os.path.join(self.npy_dir, f"{video_id}.npy")
            d = np.load(path, allow_pickle=True).item()['mouth']
            mouth = np.stack([
                d['laplacian_mean'], d['laplacian_var'], 
                d['light_intensity_mean'], d['light_intensity_change'], 
                d['area']
            ], axis=1)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©
            mouth_s = self.scaler.transform(mouth.reshape(-1, NPY_FEATURES))
            
            # íŒ¨ë”©/ìë¥´ê¸°
            pad = np.zeros((NPY_SEQ_LENGTH, NPY_FEATURES))
            length = min(len(mouth_s), NPY_SEQ_LENGTH)
            pad[:length, :] = mouth_s[:length, :]
            npy_tensor = torch.tensor(pad, dtype=torch.float32)
        except:
            npy_tensor = torch.zeros((NPY_SEQ_LENGTH, NPY_FEATURES), dtype=torch.float32)
            
        return img_tensor, npy_tensor, video_id

# --- 4. ë©”ì¸ ì‹¤í–‰: ì„ê³„ê°’ ê³„ì‚° ---
if __name__ == "__main__":
    print(f"ğŸš€ [ì„ê³„ê°’ ê³„ì‚°] ì‹œì‘ (Device: {device})")
    print(f"   - Model: LeNet + GRU")
    print(f"   - Weights: {MODEL_PATH}")
    
    # 1. ë°ì´í„° ì¤€ë¹„ (Validation Setë§Œ ì‚¬ìš©)
    if not os.path.exists(CSV_FILE_PATH):
        print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CSV_FILE_PATH}")
        exit()

    df_all = pd.read_csv(CSV_FILE_PATH)
    _, df_val = train_test_split(df_all, test_size=0.2, random_state=42)
    print(f"ğŸ“Š ê²€ì¦ ë°ì´í„° ê°œìˆ˜: {len(df_val)}ê°œ")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    if not os.path.exists(SCALER_PATH):
        print(f"âŒ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SCALER_PATH}")
        exit()
    scaler = load(SCALER_PATH)
    
    val_dataset = InferenceDataset(df_val, NPY_DIR, PNG_DIR, scaler)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)
    
    # 2. ëª¨ë¸ ë¡œë“œ
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
        exit()
        
    model = MultiModalAutoencoder(config).to(device)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    print("âœ… í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # 3. ë³µì› ì˜¤ì°¨(Loss) ê³„ì‚°
    losses = []
    video_ids = []
    # ê°œë³„ ìƒ˜í”Œì˜ Lossë¥¼ êµ¬í•˜ê¸° ìœ„í•´ reduction='none' ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ ê³„ì‚°í•˜ê±°ë‚˜ 
    # reduction='none' í›„ meanì„ ì·¨í•¨. ì—¬ê¸°ì„  ì§ì ‘ ê³„ì‚°.
    
    print("ğŸ” ë³µì› ì˜¤ì°¨ ê³„ì‚° ì¤‘...")
    with torch.no_grad():
        for img, npy, v_ids in tqdm(val_loader):
            img, npy = img.to(device), npy.to(device)
            
            # Forward
            cnn_out, rnn_out = model(img, npy)
            
            # Loss ê³„ì‚° (Batch ë‚´ ê° ìƒ˜í”Œë³„ MSE)
            # ì´ë¯¸ì§€ Loss: (B, 1, H, W) -> (B,)
            loss_p = torch.mean((cnn_out - img)**2, dim=[1, 2, 3]) 
            # NPY Loss: (B, Seq, Feat) -> (B,)
            loss_n = torch.mean((rnn_out - npy)**2, dim=[1, 2])
            
            # Total Loss (ë‘ ì˜¤ì°¨ì˜ í•©)
            total_loss = loss_p + loss_n
            
            losses.extend(total_loss.cpu().numpy())
            video_ids.extend(v_ids)
            
    # 4. í†µê³„ ë¶„ì„ ë° ì‹œê°í™”
    losses = np.array(losses)
    mean_loss = np.mean(losses)
    std_loss = np.std(losses)
    max_loss = np.max(losses)
    
    print("\n" + "="*40)
    print(f"ğŸ“Š [ì •ìƒ ë°ì´í„° ë³µì› ì˜¤ì°¨ í†µê³„ - LeNet+GRU]")
    print(f" - í‰ê· (Mean): {mean_loss:.6f}")
    print(f" - í‘œì¤€í¸ì°¨(Std): {std_loss:.6f}")
    print(f" - ìµœì†Œ(Min): {np.min(losses):.6f}")
    print(f" - ìµœëŒ€(Max): {max_loss:.6f}")
    print("="*40)
    
    # 5. ì¶”ì²œ ì„ê³„ê°’ ì œì•ˆ
    threshold_2std = mean_loss + 2 * std_loss
    threshold_3std = mean_loss + 3 * std_loss
    threshold_max = max_loss
    
    print(f"\nğŸ’¡ [ì¶”ì²œ ì„ê³„ê°’(Threshold)]")
    print(f"1ï¸âƒ£ ëŠìŠ¨í•œ ê¸°ì¤€ (Mean + 2Ïƒ): {threshold_2std:.6f} (ë¯¼ê°í•˜ê²Œ íƒì§€)")
    print(f"2ï¸âƒ£ ì—„ê²©í•œ ê¸°ì¤€ (Mean + 3Ïƒ): {threshold_3std:.6f} (í™•ì‹¤í•œ ì´ìƒì¹˜ë§Œ íƒì§€)")
    print(f"3ï¸âƒ£ ìµœëŒ€ê°’ ê¸°ì¤€ (Max Val):    {threshold_max:.6f} (ê°€ì¥ ë³´ìˆ˜ì , ì˜¤íƒì§€ ìµœì†Œí™”)")
    
    # 6. íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(10, 6))
    sns.histplot(losses, bins=50, kde=True, color='green', label='Normal Data (Val)')
    plt.axvline(threshold_2std, color='orange', linestyle='--', label=f'Threshold (2std): {threshold_2std:.4f}')
    plt.axvline(threshold_3std, color='red', linestyle='--', label=f'Threshold (3std): {threshold_3std:.4f}')
    plt.title("Reconstruction Error Distribution (LeNet + GRU)")
    plt.xlabel("Reconstruction Loss (MSE)")
    plt.ylabel("Count")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    save_path = "threshold_distribution_lenet_gru.png"
    plt.savefig(save_path)
    print(f"\nğŸ“ˆ íˆìŠ¤í† ê·¸ë¨ ì €ì¥ ì™„ë£Œ: {save_path}")
    print("ê²°ê³¼ ê·¸ë˜í”„ë¥¼ í™•ì¸í•˜ê³  ì‹œìŠ¤í…œì— ì ìš©í•  ì„ê³„ê°’ì„ ì„ íƒí•˜ì„¸ìš”.")